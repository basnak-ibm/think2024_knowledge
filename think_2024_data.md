ANNOUNCER: Please welcome Chairman and Chief Executive Officer, IBM, Arvind Krishna. [ APPLAUSE ] KRISHNA: Good morning. Good morning, everyone, and welcome to Think 2024. It's wonderful to see you all here, and it's incredible that so many business leaders, technologists, innovators are here, are here so you can all learn from each other. When we look at Think, Think is really meant for you, our clients and partners, to help you co-create with each other and together with all of the technologies that you going to see. and I do hope you'll get a chance to walk around the Expo floor where there are so many of our team members and demonstrations there for you. I think this is a critical moment in technology. When we look at the confluence of hybrid cloud, artificial intelligence, quantum computing coming down the road, all of these are technologies that are reaching maturity that can really impact the business and really help the business improve. They can boost productivity, they can bring you automation, they are bringing you innovation, they can help your business at scale in a way that couldn't even be imagined a few decades ago. We're really proud of our technology Atlas -- a concept we introduced last year -- where you can begin to see how these technologies are going to evolve, and you can fully experience that at Think on the Expo floor where we're going to show it to you and show you how much progress that we have made. Now, the topic that is really in everyone's mind, artificial intelligence. If we look at the history of the world on technology, every generation...well, maybe every half a century or so we have had a fundamental technology that has helped the world really advance. You can look at examples: the steam engine, or electricity, or the most recent one, the Internet. What did they do? They made global GDP go up; by the way, with the steam engine in about a century, it went up by 10 times. Again, if we look at electricity, the same thing happened circa 1870 to 1950. If we look at the Internet, it is well on its way there; this time, looks like about 50 years will get us that 10 times increase. Artificial intelligence should be thought about in the same way as those fundamental technologies. It's only going to take a few short years, and we will find it infusing everything. The current estimates is $4 trillion of annual GDP productivity by 2030. Just think about that: over $4 trillion associated to one technology. Now, that's not the technology spend, that is the value accrued to all those that are using it. What's needed to make this come real? There's a lot of experimentation that's going on. That's important, but it's insufficient. If you watch every one of the previous technologies, the history has shown you kind of move from innovating a lot to deploying a lot. As you deploy it is when you'll get the benefits. But in order to deploy, you also need to start moving from experimenting to working at scale. Think about a small project. Then you've got to think about it in an enterprise scale in a systemic way. How do you begin to expand it? How do you begin to make it have impact across an enterprise or across a government? And that is what is really going to make it come alive. When we look at the stage right now, incredible excitement in the consumer world. We see these apps, we see these APIs that are able to impact hundreds of millions of consumers. And I think that's wonderful, helps the technology advance, gets people used to it. You can see the excitement. I kind of think of it as equivalent of maybe browsers, since I'm using the Internet analysis. But then, if you think about it in the Internet analogy, really unlocking productivity inside an enterprise, how do you really connect everything from your inventory to your supply chain all the way to the front end and omnichannel? How do you begin to connect all those systems? AI is going to have the same impact. It's going to become deployed across the enterprise, unlocking and unleashing a massive amount of productivity. For the first time, we will have assistants that really augment humans able to work together with us, unlocking all this. So, what do we need to be able to unlock all these benefits? And if we think about the scaling, that is why we introduced watsonx last year. If you remember, we talked about it and we launched it in the month of July last year -- so about 10 months in market -- and it really is aimed at helping you accelerate your AI deployment. Because why? You need an environment where you can trust, you can play with models, you can experiment with models, you can fine tune models, you can add your own data and skills to models. As our team tells me, a model, in the end, is just a representation of your data. So, in some sense, all your IP is getting locked up, but in a slightly different form, so you need that trust to decide where are you going to run these things, where do you train them with your own data, how do you make sure that only trusted data, what kind of guard rails do you want on it? All of those pieces are going to be there. We also want to make sure that you can leverage smaller purpose tuned models. Why do I say that? When we are looking at the energy, at the cost, at the overall environmental impact of running very large models, the question comes up, can a small model do as much maybe using one percent of the energy and one percent of the total cost? No compromise on quality, no compromise on accuracy? If we can do that, then there is a massive advantage. Of course, I'm saying "if," because we are a hundred percent sure with some of our learnings that that is possible, and I'll touch on one example in just a minute. So, can we control those skyrocketing inference costs but maintain the quality and performance? And also, in many cases we want to bring expertise, bring you people who have worked on many projects before that unlock massive value for the enterprise and bring you their expertise to deploy it at scale inside, touching on the technologies, but also some of the people changes that are needed in order to embrace all of this. The use cases that we see going across our thousands of clients fall into three buckets. They fall into customer experience, into code -- as in, programming -- and into digital labor. Sort of some quick examples to make them come to life. When we think about coding for a moment and we think of what we're able to do ourselves in some cases for Java, for COBOL, for Ansible, our teams are able to get 60 percent of productivity, but I'll be cautious: in the programming task. The remaining parts that still touch the business that get requirements are still very much all human. But 60 percent productivity in half your time I think is a pretty good number. One of our clients, Broadridge, used these tools to improve an application -- they're in the fintech space -- and they saw their programmers get a massive amount of productivity as they evolved forward. When we think about customer experience, rather than ourselves, one of our clients -- in this case in Europe, a telecom provider -- was using Watson to handle 800,000 calls a month. What they found is 40 percent improvement in customer satisfaction means people were happier interfacing with the artificial intelligence -- in this case, powered by watsonx, -- but also 30 percent time reduction on average in terms of how long it took for people to get their answer. So, when we think about these great examples on code, great examples on customer experience, and I think with digital labor, our own human resources center managed to get now 95 percent of the queries -- what we call the transactions that people ask -- get handled between automation and AI. I think these are great, great, great examples. So, the first piece of news that I want to talk about, we decided that we are going to Open Source our Granite models. So, Granite is a family of models that IBM is creating. These are large language models that we have invested in now, and we have them in language. We have them in code, and I'll touch on the code example in just a moment. We have them for time series. We have them for climate change. We're beginning to put them out for cyber. And when we say language, there's many languages that we will do. But we decided that we want to Open Source this. They are ranging anywhere right now from three billion to 34 billion parameters, so that's kind of the range they're in. There isn't a fundamental limit. They could get larger, they could get smaller. We believe that actually having a smaller model, but that is tuned for a purpose -- so, rather than one model that does all these things, if we can actually make models that are fit for purpose, then we can give you the best of both worlds. What do I mean by the best? Look at the leaderboards to say, how good are they? So, we take pride in how good our code model is. It's up there on the leaderboards. It'll go match the best in the world, regardless of how big they get to be. And that's kind of our commitment, and we have 116 programming languages already in there. But then on languages, we'll do some ourselves, but we'll also embrace those who have their own great languages. And so these are put out under the Apache license. Why Apache? So that those of you who want to add a skill from your own data and your own domain knowledge, then the Apache license allows you to keep that to yourself, and you don't have to share it unless you want to. But if you want to share it, then it can become shared, and everybody can benefit from it. Now, other than this, we are also going to be bringing it into a lot of assistants. We have the watsonx Assistant for Enterprise Java that's going to be coming out later this...late in the summer. We have the watsonx Assistant for Z, which allows people with no skills on Z to very quickly learn and get productive on that platform. We have the watsonx Assistant for Code, in which case this is one that is on COBOL, another one that's on Ansible. And also an Assistant Builder so you don't have to rely upon the assistants that we provide, you can actually build your own assistant very quickly. Anybody interested? Go look at them on the expert floor. All of these are out there, and our teams will be excited to show you what to do over there. Now, as the environment becomes bigger, everybody is using multiple public clouds, people have SaaS applications, people have their own in-house applications. With generative AI, estimates are between 600 million to a billion new apps will be written by the end of this decade. Just think about it. So, think about all of that sprawl and all of the connectivity and all of the infrastructure under all those. We feel that it's time to bring AI to IT operations and really help you get a sense of what's going on, what is happening when something goes wrong. Can you judge your response time? Can you take care of problems on their own as opposed to putting humans in the loop? And that is our vision behind IBM Concert. Starting step, looking at all of the security implications on the environment I just described, where do you need to patch, where are you getting something missing, what do you have to go fix very quickly? But from there, the whole vision of what I described is what you are going to see us rolling out over the next many months. My final piece of news around our ecosystem. This is a place where we put a lot of investment and we have really focused on both partnerships and integrations. If you look at what we're doing with OpenShift and watsonx with Adobe Experience Platform, I think that's a great integration. We're going to talk a little bit more about that later. Our Consulting teams are also doing a lot of work on Adobe Express. When we look at AWS, we are integrating the watsonx family with AWS SageMaker, especially for governance -- another great, great example of partnership which is going to help drive business, I believe, for both AWS and IBM. watsonx runs on Microsoft Azure, the whole platform. It is in the Azure Marketplace, so that's a great example of what is there. Also, many other IBM products in the Azure Marketplace. In the case of Meta, Llama 3 is now available through watsonx with indemnity back to our clients. That's a great example, again, of us coming together. Mistral -- a partnership done so that the large Mistral models can be purchased through watsonx. SAP, ServiceNow, Salesforce, integrations of our Granite models with all three of these platforms, that's all been announced or getting announced very quickly. And my last example, with the Saudi Data and AI Authority -- SDAIA. SDAIA did a lot of work to create a wonderful Arabic model, ALLaM. And we have just recently signed an agreement to bring ALLaM, the Arabic model, to watsonx. So, joining me on stage to talk about that, I'm honored to have His Excellency, Dr. Esam Alwagait, the Director of the National Information Center at SDAIA, to join me and we'll talk about the ALLaM model. [ MUSIC, APPLAUSE ] Esam, welcome. Welcome to Think 2024. ALWAGAIT: Thank you very much. I'm very glad to be here in such a remarkable event. And we're very thrilled with the announcement of integration of our Arabic LLM, ALLaM, into watsonx. KRISHNA: So, Esam, maybe you tell the audience a little bit about SDAIA and SDAIA's mission. ALWAGAIT: Arvind, we stand at the edge of a new era. The transformative power of AI is evident around the world; and in Saudi Arabia, we realize this more than ever. Our vision is to harness the potential of AI, to drive economic diversification, to enhance government efficiency, and to provide quality of life for all. In the Saudi Data and AI Authority, we stand at the forefront of Saudi's AI endeavors. We look at a wide spectrum from innovation, to regulation, to operation, alongside a wide range of national ecosystem and partnership. To achieve our vision, we develop and operate critical government digital services and infrastructure that stands on top of a national data bank -- a bank of interconnected national data platforms that aims to increase data sharing, data quality, and to instill data as a main driver for the digital economy in Saudi Arabia. Arvind, let me give you one example of our product. It's called sawtuk, which means "your voice." Sawtuk understands accurately more than 20 dialects in Arabic, and we plan to add more languages in the pipeline. This will improve the human-computer interaction. There is a limited, if not a lack of, focus on Arabic language in LLMs, and that was a call for action. And our action was ALLaM. KRISHNA: That's great to hear. So, when we think about this announcement and what you have done with ALLaM, how will ALLaM and watsonx coming together contribute to your mission to be a global leader and to fulfill the objectives that you just set out? ALWAGAIT: Well, watsonx has many capabilities; for example, enabling LLM development, simplifying machine learning operations, providing the flexibility for hyper cloud deployment. And now with the addition of ALLaM -- the Arabic LLM -- into the platform, we believe that this is a major leap of our goal to becoming an AI leader. In SDAIA, we have many AI products, but ALLaM is our gen AI solution that focuses mainly on Arabic language. We know that rich LLMs are the basis of successful gen AI deployments; however, current LLMs have some limitations when it comes to Arabic language. That's why we provided ALLaM. ALLaM was trained on more than 500 billion tokens in Arabic, and this is just the beta version. We'll continue improving. We'll continue training it to more Arabic data sets. We'll continue to fine tune it, to reach our goal, to have ALLaM as the leading Arabic LLM model in the world. And we know, Arvind, that IBM has a reputation of having high standards when it comes to quality assurance, and we're very happy that our product has passed your QA tests. KRISHNA: It's great to be together on this, but I think you'll agree... [ APPLAUSE ] But also on the other side, who could have a richer trove of Arabic literature and material than Saudi Arabia? ALWAGAIT: A hundred percent. A hundred percent. KRISHNA: So, what is your vision for our partnership going forward? In what ways should we collaborate even more? ALWAGAIT: Well, SDAIA's an IBM's partnership as a strategic one, and it has proven itself over the decades, and today's announcement is a landmark in this strategic partnership. When it comes to AI governance and ethical AI, we share the same priorities and concerns with you all. In SDAIA, we pay huge attention to ethical AI. That's why we launched our second version of Ethical AI Framework, to enable people and entities to develop AI-based solutions that are safe and trustworthy. We're also present on the global stage and we collaborate with many international entities; for example, we've collaborated with the Global AI Advisory body in the UN to reach a vision in a more governed AI. We also got the recognition from the UNESCO's General Conference that granted the International Center for AI and research in Rihyad the status of a Category 2 center. Also, SDAIA's president, Dr. Abdullah Alghamdi, participated in the AI Safety Summit 2023 in the U.K., trying to reach to a more common understanding of AI risks and to have an interesting collaboration on AI safety matters. Arvind, watsonx has AI governance built in it. As we progress and develop more guidelines, more regulations, more standards in AI, it's very important to have the ability to enforce them through code, and having this ability and new platform is a huge advantage. KRISHNA: So, first of all, Dr. Esam, thank you so much for partnering with IBM and all of the collaboration actually over the decades that we have done together. Any final comments from you? ALWAGAIT: Well, I would like to thank you, Arvind, and thank IBM for allowing us to participate in this marvelous event. And I would like also to thank His Royal Highness, Prince Mohammed bin Salman bin Abdulaziz Al Saud for his unwavering support for SDAIA that enabled us to develop AI products, and today's announcement is the result of that support. I would like to invite you and everyone here to attend the third edition of the Global AI Summit in Riyadh in September this year, where we have the pioneering innovators, the pioneering policymakers, the thought leaders to explore new horizons in AI and to forge meaningful collaborations to advance AI forward. Thank you very much, Arvind. KRISHNA: Thank you. Thank you. [ APPLAUSE, MUSIC ] Let's thank Dr. Esam. [ APPLAUSE ] I think that's a great example of a rich LLM and two people who trust each other coming together to bring all the innovation to the world. But to go a little bit deeper, to get the productivity that gen AI promises, we need three things: one, we need to trust the underlying artificial intelligence, and you heard a little bit of that with Dr. Esam. But that is the reason that we together with Meta help form the Open AI Alliance where more than a hundred organizations have gotten together across industry, academia and government in order to help form standards and tests so that we can trust AI. We also need AI to be flexible. How do we combine models? Models should come whether from parties like us, whether from other parties like SDAIA, whether from other commercial entities, whether from Open Source. And you should all have the flexibility to deploy these models where you see fit on a public cloud of your choice, on premise, with sovereignty or in a large public environment. And so how do you get all of that flexibility? And lastly, safety is critical, and we fundamentally believe that safety comes when many eyes can look at something, otherwise called Open Source. So, how do you really get many, many more minds on this which brings you both safety, but also innovation? That's one of the reasons that we've been pushing on bringing a lot of capabilities around AI inferencing and deployment into Red Hat Linux, and Red Hat Linux has a lot of the capabilities on how do you form a platform to run these AI models and run inferencing tasks across many different hardware, but optimized to the hardware so that you can extract 10 times more performance than you might be able to get otherwise. We include the Granite models, but we also include many of the other open source models that I just talked about. But then you get the question of, how do you begin to add skills? And I'm using the word "skills" purposefully, and capabilities. How do you infuse some of your own proprietary data and knowledge to add a skill where a model that is built may not have something that is for a particular purpose. You could imagine cases around certain languages, around code and so on. So, you need to infuse it, but you need to do it quickly at much lower cost than other approaches, and that is why we are bringing InstructLab to the market also. Now, rather than me talking about InstructLab, let me bring out one of the software engineering managers from Red Hat, [Mo] Duffy, to talk to all of you about InstructLab. Mo, welcome on stage. [ MUSIC, APPLAUSE ] So, Mo, thank you for joining us. Can you start by telling us on InstructLab, what's new from a client's perspective? DUFFY: It's a great question. So, when I like to understand how a technology works, I like to start with the client problem to solve. So, we'll take an example of an insurance company. Here we have, whoops, look at all that red. Those are insurance claims waiting on repair recommendations for an auto claims service. Now, this insurance company has lots and lots of historical repair recommendations for claims in their databases. Why can't they take advantage of that and see if we can improve that, get that red back to green? So, what we ask ourselves here, can we use AI and teach a model all about those historical repair claims that worked in the past and use it to create recommendations for the future to kind of unblock all that red? KRISHNA: But how do you get all that knowledge into the model? DUFFY: That's a great question. So, we have the lab method, which was invented by IBM Research, and this is the engine that drives InstructLab, which Arvind has already introduced to you. So, InstructLab has four components. The first component is a taxonomy based tree of knowledge -- so, as Arvind said, we have skills that we want to teach the model. We'll use the client's knowledge, infuse them on top of an open base taxonomy tree. That will be used to do synthetic data generation, we'll talk about that a little bit later. Once we have that synthetic data generated, we will critique the data to validate it. And then once we have that data set, we'll use that to fine tune a model. That model will be fine-tuned with the client's data, and they will have their own version of an open-source model based on their knowledge. KRISHNA: Sounds really, this thing, you used this word "taxonomy" a couple of times. What's a taxonomy, and how do you augment a taxonomy? DUFFY: Sure. So, basically, we're going to take...the taxonomy is a tree of knowledge. This is opposed to the blender method of AI training, where you just take all the data that you can suck in and throw it in a blender and see what comes out. This is a more ordered form of constructing your data, which will help you identify any gaps in the data. What we suggest doing is taking the open community, the innovation that the community has provided us, use that as a solid base; and then, the customer can create their own private version of the tree using that base, infusing their knowledge into it. So, what we do is, for our insurance company, we will take their claims data and create a question and answer file. This is basically a quiz that we use to teach the model all about, what are the best repair recommendations to make for this type of car, that type of accident, that kind of stuff. And then we have a custom tree built just for that client. KRISHNA: So, do I remember this right, so sort of an engineering approach, but also a lot faster than prior approaches of what we'll call fine tuning. Is that accurate? DUFFY: That's right. So, what we're doing here is, you know, this is an insurance company, they have a certain amount of repair claims data. They might not have enough data to actually move something as large as a language model. So, what we do is we amplify the data that they have using the synthetic data generation technique. This way, it makes it faster. We don't have to rely on human labor to expand the data set to move the model. It just processes that using the algorithm that IBM Research came up with. So, it's a cost-effective way to move the model. Once we have that data set, our imaginary insurance company here will have their own custom repair claims data recommending model; and then once they have that model trained on their data, they can add that into an app. Here we have an application that is meant for insurance claims agents to suggest repair recommendations to them. And here we go, it's making a great recommendation based on the repairs data that we have. We've unblocked all of that red, getting it into green. Our client can better serve their customers. And this is all a great example of the power of open. We have open-source licensed models, we have open-source technology in InstructLab, and we have that open taxonomy base for the client. So, all using the power of open. KRISHNA: All open and months of work converted into a few days there. I think that's a great, great example. Mo, thank you so much for sharing InstructLab with us. DUFFY: Nice to be with you. [ MUSIC, APPLAUSE ] Months to days, I think that's the kind of innovation we all want to see in artificial intelligence. So, I wanted to start sharing some real world success stories, and the first guest that I have coming out is going to talk to us about some of the incredible work that Citi, a global bank, is doing around AI. And I want to discuss the evolution of AI as Citi under the leadership of Shadman Zafar, who is the Citi Co CIO, and he's responsible for technology and digital ops behind all of Citi's customer solutions. Shadman, can I welcome you to the stage? [ MUSIC, APPLAUSE ] So, Shadman, it's a real pleasure to have you here with us. To start, could you give us a high-level overview of Citi's AI strategy? ZAFAR: Of course, Arvind. First of all, good morning. It's an absolute pleasure to be here. What a show, by the way, what a show. It's fantastic. Talk about AI strategy for Citi, let me start with an obvious. So, Citi is a large bank, global bank. When you're a large global bank, what you do, you sell money, and the great thing about selling money is your product is always in demand. You know, if you don't have money, you want money. If you have money, you still want more money. [ LAUGHTER ] So, when that happens, how do you differentiate? I mean, it's a great thing, but how do you differentiate? It really comes down to three things: number one, responsibly acting towards your money. That's about risk and control and governance, how we manage money well and how we earn trust from our customers on a daily basis. That's the foundation of our business, and that's one of the pillars about AI strategy. Second, friction: how easy it is to do business with us. More friction, harder for us to do business with, you can go to somebody else for that. Third is a scale. Banking is a scale business. So, our AI strategy is built on these three principles primarily. First and foremost, we are focusing on utilizing AI to really enhance, automate and strengthen our controls. In a financial industry and for a large bank, you're going to have tens of thousands, hundreds of thousands of controls at any given moment in time, monitoring, tracking, managing, remediating issues that may happen. And imagine taking those with AI, and we are looking at making them automated, enhanced, complete, robust. I'm a big believer in having not thousands of controls but a few good controls, and AI is really helping us do that. Second, as I said about friction. We are looking at both for ourselves and how we service our customers, taking friction out by simplifying our processes, automating activities that are generally not easy to do with very brittle processes, if you will. The third, as I said, scale. Many companies have taken the approach, Arvind, in AI of like, let's figure it out, we're going to put it in a lab and do something. We don't think about it that way. We have this mindset, we say take AI out of the lab on to the factory floor, so we have deployed it to tens of thousands of people to use it in everyday capabilities, if you will. And those three pillars of strategy, if done well, have a side effect. If done well, you get a massive productivity, because if you take friction out, you automate controls, you obviously get a huge productivity and we sort of turbocharge our employees with that. KRISHNA: I love your point of scaling. I think it is so, so important that our audience also recognizes that unless you scale, the full benefit of AI is not going to come. You'll get some, but not the full. ZAFAR: That's exactly right. KRISHNA: So, tell us a little bit about the collaboration between Citi and IBM and what kind of solutions you think we're working on together. ZAFAR: Sure. So, you know, Citi and IBM relationship is deep. It's actually deeper than I even realized recently. It goes back many decades, I think to when I was still in high school or probably middle school. And before I was coming here, I was actually looking and trying to look for some examples of where we don't work together, and I literally couldn't find one -- because I was trying to find an example to say, and this is a new area we'll work on. But be it AI, cybersecurity, cloud computing, enterprise computing, legacy systems, modern systems, even business processes. We worked with IBM for a long time, long before my time, by the way. I think, Arvind, long before your time, too. KRISHNA: Probably. [ LAUGHTER ] ZAFAR: We have worked together well. So, bringing that to...so, because of that kind of deep relationship, obviously we're deeply intertwined in our AI path as well. We are very focused on AI. I'll give you a couple of examples how we are working together. So one, let me start by sort of hardware side, and then I'll talk about business processes. So, again in banking, fraud is a big area from a compute perspective. You have to figure out there are, you know, millions...sorry, billions, tens of billions of transactions happening every day. Figuring out what happens, detect the fraud, and then do something about it. It's a very near real-time activity we need to take place. The way it happens today, Arvind, is you have to collect all of that data, you have to offload it for like a machine learning algorithm to execute, and then bring that data back on after there is a score to take action about it. In the case of fraud, that velocity matters. So, one of the key things we actually worked with IBM is IBM's new Telum which has AI onboarded, and it executes with a very significant inference capabilities while...I mean, the interesting part was, at least for us when we were working with your team, is to really see that we can actually onboard and offboard in real time AI inference execution and sort of traditional processing all together in one play. And we have now convinced ourselves can actually do this whole thing in one place in real time. KRISHNA: There was skepticism about it originally. ZAFAR: I mean, I've got to tell you, I was a skeptic. I was a skeptic, my team was a little bit of a skeptic. And it was fantastic to see we can do that in real time, not offload it, and also train on production data. So, that's fantastic. I mean, that is I feel is like a step function change for us. But also we are working on, we are an Ansible shop. I mean, you talked about Ansible. We're an Ansible shop. And a company like Citi, you know, at any given moment in time there are tens of thousands, hundreds of thousands of changes happening on our infrastructure every week. And we are a mission critical company. So, we are actually working on Ansible with watsonx integration to really automate that thousands and tens of thousands of jobs, make them sort of automated, real time, and improve the quality of our technology operations. And I'll just give you one last example because this sort of touches a different area. As you know, Arvind, we use OpenPages from IBM for our risk and control governance platform. And again, with the watsonx integration coming in place, we are actually building now a new set of tools to use AI to write in from auditing perspective, audit report, reviewing, figuring out the risks, recognizing the risks in real time, and being able to take action on them. So, there's a long list of things... KRISHNA: I really love those examples. They do play to your scale point. I mean, audit, if I remember, you have four, five, 7,000 people in those departments. ZAFAR: That's very true. KRISHNA: And on code deployment, there's tens of thousands of engineers who work for you and your teams... ZAFAR: That's correct. KRISHNA: ...who get impacted. I think that these are great scale examples. So, maybe as a final question, are there other use cases you're considering; or kind of, you know, I'm sure you get asked this question by your boss, what's up next? ZAFAR: [CHUCKLES] Every day. Every day, seven days a week. So, look, first of all, as I said, we are approaching AI not as in a lab two use cases or five use cases, we are really looking at it as a new technology stack. So, we are looking almost everything we do through that lens, a new technology stack; and because of that and thousands of programs we look at, that sort of approach through AI lens, if you will, Arvind. But to maybe take a step back, first of all, Citi has been utilizing AI for some time. I mean, even before sort of generative AI energy that came into the market, we have hundreds of sort of traditional AI models in place to do fraud detection, natural language processing to respond to our end customers. We even use like AI-integrated end device protection. We have like three quarters of a million devices that are protecting ourselves, utilizing AI against ransomware and whatnot. But yes, to give you a sense, at a high level we look at major patterns. So, four patterns I will highlight, is like a pattern around smart query. So, we are trying to get most of the corpus of corporate data into sort of a RAG environment that you can in a very simple way ask questions. And it can be used multiple ways: internally to create documents, to produce reports, to whatnot, but also externally to provide services to our end customers. Many times our front end employees are sifting through thousands of documents to actually figure that out. So, that's what we call smart query. Then you have sort of the smart content generation. Again, for internal use and external use, they're using generative AI to produce sometimes external reports, sometimes internal documents to sort of create and analyze things. Then it's around really engineering itself -- so, code modernization -- and there's a long list of things there, actual code assistants and code writing but also taking old systems and modernizing them to new systems. By the way, those are projects that will not just financially but logistically impossible to undertake how complex and how old the systems were that we can undertake now. I've tried it seven, eight times before, and I always fail, and I'm finally actually seeing the success happen, and it was not because of lack of money, because it was just logistically so hard to undertake a project like that. And the last I would say -- and that's probably the most common we all hear about -- is like, you know, a set of what you would call probably copilots or general assistants, AI-based assistant that goes across the corporation for specialized functions for people. KRISHNA: I love what you talked about, you talked about a new tech stack, you talked about scaling, you talked about the user generative. But it struck me, you also gave an example that you can now do something that actually even with money and people before you would not have agreed to tackle but this technology makes that possible. ZAFAR: Absolutely. KRISHNA: I know exactly what you're talking about, when there's a spaghetti mess of 10 million lines of code, how do you unravel it and do something with it? I think those are great examples. Shadman, thank you so much for being here... ZAFAR: All right, thank you. KRISHNA: ...and sharing your story with us. ZAFAR: Of course. Thank you. KRISHNA: Thank you. [ APPLAUSE ] Look, as we think about the example from Citi -- and I hope that you all got some great ideas of what you can all do -- I think it's time to maybe also talk about one of our great partnerships, a partnership that was founded on strength but has been strengthening again and again, and that is with Adobe. IBM was an early, early adopter of Adobe Firefly, and I will tell you that our teams love it. When we think about what our teams were telling me in the middle of 2023 as that were using it, this is a great example of how AI is strengthening a product that they liked already. By the way, it's not just our teams inside IBM, though they love it, but our Consulting teams have also been deploying the Adobe Experience platform, and they're also creating new service offerings for Adobe Express. So, to help talk about all this, please join me in welcoming Shantanu Narayen, CEO and Chairman of Adobe, to the stage. [ MUSIC, APPLAUSE ] Shantanu, thank you for joining us here. NARAYEN: My pleasure. Thanks for having me. Wow, quite an audience here. KRISHNA: It's a great audience, testament to the interest in AI, I think, actually. So, Shantanu, thank you again. So, the Adobe/IBM relationship has seen significant growth over the past few years. Any comments on that, and what do you see as kind of what's behind that? NARAYEN: Well, again, thanks for having me. And as you said, I think we as companies have a passion for how do you have technology and how do you put technology to bear to impact people's lives. You talked a lot about productivity, Arvind. I mean, certainly Adobe perhaps focuses a little bit more on creativity. And I think, you know, what we have done together, a lot of people...for Adobe, we have really three key businesses. We talk about enabling people to create content, we talk about the role that PDF plays as it relates to information and how you can have knowledge management. And perhaps a business that people know us less for is powering digital businesses. So, as you know, we were one of the first companies to say we're really going to focus on marketing and how do you enable people to have those personalized digital experiences. In all of these areas, we've been really thrilled to partner with IBM. IBM was the first company to also talk about how they would be a global partner for us with Express and really getting Express, which is a creativity tool, out. But I think the thing that we've probably done the most pioneering work in together is content supply chain. We all know the world's producing way more content, so really thinking about how do you create that content, how do you deliver that content, how do you monetize that content? You've announced a global partnership with us on that, so it's been really exciting. But I think the best is yet to come. KRISHNA: Look, I think it's exciting. I think what you just related, Shantanu, is that we have a combined mission of digitizing, in this case, I'll call it the creative supply chain inside an enterprise, even though we market to enterprise customers. And being able to share that vision, I think is behind some of the success we have had. Now, another area other than AI that we both have a belief in is the technology called hybrid cloud. Would you talk a little bit about how Adobe has been adopting Red Hat OpenShift and how it empowers you with your clients? NARAYEN: Absolutely, Arvind. I think you talked a lot about, you know, if you think of the stack that exists for AI, you talked about data and the importance of data. You certainly talked about models, and I'm sure we'll touch on that, and interfaces. One of the products that Adobe really focused on a lot was what we call the Adobe Experience Platform. And the Adobe Experience Platform is really a real-time customer data platform that allows you to activate in real time any customer engagement that's happening. So somebody coming to your website, somebody being in a store, how do you really activate that? Now, we know that every one of the customers in this room has a very heterogeneous environment, and so allowing flexibility. You talked about regulated industries, financial services. So, I think it was really critical for us to partner, to understand how we could get the Adobe Experience Platform to work in this heterogeneous environment. And certainly OpenShift was the best way for us to embrace so that we could get it to work, regulated industries, public sector, getting it to work on AWS. So, we're thrilled to partner and have the Adobe Experience Platform available through Red Hat OpenShift on all these heterogeneous environments. KRISHNA: That's great. So, opens up the aperture to get even more business done at the end of the day. NARAYEN: And to really allow all the customers in this room to say we have an environment. IBM, Adobe, you need to partner to ensure our environment is the way we deploy these solutions. KRISHNA: So, look, Adobe has long been known as a leader in artificial intelligence, but how are you integrating more generative AI within your products and taking advantage of this for your clients? NARAYEN: In your opening remarks, you went back perhaps a little bit longer than I would have when you talked about steam engine and electricity, but I certainly agree with you on the impact that this can have. And like Shadman also said, Adobe's been involved, I mean, computers are great at what? They're great at pattern matching and AI. And so we've always embraced every tectonic technology shift to say how we can bring value to our customers. But when you think about creativity, I think the biggest challenge a lot of people have is that fear of a blank screen, right? A blank page. And so we really just think about generative AI. We say that everybody has a story to tell, and if we can enable people to tell their story through our creative products, whether it's Express for every individual, we just think that's going to change everything as it relates to AI. So, Adobe announced Firefly, that's our imaging model. We're very differentiated in that we said, because we represent the creative community we have to make sure that our models, like you said indemnification, we actually indemnify because we say all of the IP on which these models were trained for Firefly are data that we own. So, we have not violated any IP protection. So, we did that on the models. And then in the interfaces, you know, I've been at the company for over 25 years, but the adoption of this in a product like Photoshop where people have said through generative fill we can now just have a conversational interface with Photoshop and do things, it's just mind boggling, the kind of innovation that we've done. So, that's on the creative side. On the marketing side, every one of you is probably saying, how do I provide that great personalized experience to a customer? How do I segment them? How do I acquire them? How do I do geographic variations in different countries? And I think that's where we've really embraced the power of generative AI, to accelerate, you talked about automation, so whether it's automation, whether it's creation or whether it's delivery, we think it's going to be massive productivity gains. KRISHNA: But in the end, products are becoming better, and the clients are able to use them more... NARAYEN: Exactly. KRISHNA: ...is what you're kind of saying. You mentioned indemnification, but the broader topic behind indemnification is really trust, and I think we both agree that building trust behind how we use AI and deploy AI is critical. Which strategies do you think are essential that our clients believe that we are ethical AI producers? NARAYEN: It's a really important topic, because for companies like IBM or Adobe, the biggest thing that we have is the trust that we have with customers. So, a couple of things that we have done on that front, Arvind. First, as I said, the Firefly models, every piece of data that we've used to create those Firefly models is data that we have intellectual property rights for. We certainly have partnered with every company to say, if you have your own data...you talked about how you can create these custom models, so we enable people to create those custom models with trust. But I think taking a step back, we thought a lot about, since we helped create the world's content, what do we have to do? And we created this new concept called Content Credentials. We're fortunate we have over 3,000 companies that are now part of this consortium. And the idea behind Content Credentials, think of it as a nutrition label for a piece of content. And what you can do is for every piece of content, you can specify when it was created, what's the provenance, did AI change it, so that people are assured of...I don't know if people know, 50 percent of the world's population is going to be voting in a 12-month period. And when you talk about all the fake news or content that's created, I think that's one area where we've tried to step up and say, how do we act responsibly? How are we transparent? And at the end of the day, how do we make sure there's accountability for all of the AI work that we are doing? KRISHNA: I've got to give a plug for some of these tools. Our teams have so much trust and get so much creativity out of them, tasks that used to take two to three weeks a year, year and a half ago, the teams are now getting done in a couple of evenings. That really, I think, speak to the power but also the trust that we have. So, Shantanu, any final thoughts? How can IBM and Adobe co-create going forward so that our clients can get even more value from us? NARAYEN: How much time do you have, Arvind? [ LAUGHTER ] I think there's so much we can do together, and we're really excited. I mean, you talked about watsonx. As it relates to Acrobat, let's talk about PDF for a while. We would all I think acknowledge that a lot of the world's information is in PDF. We recently announced Adobe Acrobat AI Assistant, so having a conversational interface where you can query a PDF. You can say, let me get the summary. You can now compare documents to say, if I'm reading two financial reports, what's the difference? I think what you're doing with your models and the ability with watsonx to have native models, ensuring that we can have support for Acrobat and AI Assistant with watsonx, I think that's a big, new exciting area. We all talk about productivity and productivity office tools; creativity, I think, is critical. We're thrilled that IBM has adopted Adobe Express, and you're going to be now a global partner for us to enable everybody to understand how they can create. Redshift, we talked about the things we're doing on the technology with hybrid cloud to enable even more rapid adoption. So, I think across our businesses there's so much more that we can do together, but I think the ultimate test will be delivering value to these customers. So, I think the more we're engaged with everybody in this room to say, are we getting the them the business outcomes, I think that's how we deliver value. KRISHNA: Look, we really look forward to that. Shantanu, thank you so much for sharing what we do together, and thank you again for being here with our audience. NARAYEN: Thanks for having me. Have a great show. [ MUSIC, APPLAUSE ] KRISHNA: Look, I think as you go across what we just heard, this is a really exciting moment in technology. As we look at the intersection of hybrid cloud, artificial intelligence, we really think there's an inflection where every one of you can take advantage of it to help improve your business. I think there's a macro trend that underlies this. Technology has always been used for productivity in automation. Think of it as helping your enterprises become lean. But I think there's a shift that is happening. The role is no longer about just being lean; the role has really shifted to, how is technology powering the business to gain revenue, to gain scale, to get even more share in the marketplace? And that is a big shift, albeit has been a subtle shift underneath. And I think the goal for every one of you should be, how do you embrace technology in the context of your business to help power that shift, not just make yourself the leanest possible? Now, there's a topic that is dear to my heart that I just need to mention for a minute or two, which is we talked a lot about artificial intelligence and hybrid cloud, but another topic that is just coming around the corner is quantum computing, and I think quantum computing holds great potential for what we can all get done. First, a moment of pride. We have built over 70 actual quantum computers, these systems have been deployed globally over the last half dozen years or so. We have 250-plus organizations who participate very actively, hands-on in our Quantum Network, some of you in the room because I recognize you. People have run over three trillion -- three trillion -- individual experiments on our quantum systems over the last many years. I think that speaks to the reality of these technologies. When you run three trillion of something, it's no longer a science experiment, it is something that is becoming a real computer system. As we look upon this, what's the promise? There are problems that classical computers can never solve. I'm making a very strong statement: never. We look at problems around fertilizers, food supply, around new materials, around financial risk, there are issues here that quantum computers will be able to solve in the next three to five years. That is something which really excites us. But there's a booth downstairs. You can go look at it. You can go talk to our experts who are there in terms of what can be done. So, with so much potential, most of you are on artificial intelligence at this conference, but bracketed by both hybrid cloud and quantum computing. I really would like to end by thanking all of you for your trust in IBM, your spending the days here with us. I hope you'll get a chance to interact with each other and learn something which you can take back and implement in your organization. Thank you also to all the partners who help convey all of these capabilities to our clients. But I do encourage you, make the most of this event, interact, co-create, learn something, go back with an insight. Thank you. Thank you all.





ANNOUNCER: Please welcome Senior Vice President Software and Chief Commercial Officer, IBM, Rob Thomas. [ APPLAUSE ] THOMAS: All right. Welcome back to Think 2024. It's been an incredible morning. You've heard about how we're open sourcing Granite models. You've heard about a new line of watsonx assistants. But we're just scratching the surface, because automation is upon us. Think about the last number of years how fast technology changes. And what always happens when technology changes? Ah, there's a little bit of complexity that shows up. It may slow us down, it may prevent us from doing what we want to do. Let's think about public cloud. Everybody was incredibly excited about public cloud, still are today. But then it got really difficult. The answer became hybrid cloud. That became the easy way to marry your private cloud and your public cloud. Now let's go to AI. A lot of excitement around AI, as there should be, but the number one thing clients raise to me is, well, I do have a little bit of a data problem. My data is in a lot of different places. This is why AI actually requires a hybrid architecture, because your data is going to be in many different places. So what's going to fuel these two trends? That brings us to automation. Let's think about what happens when we get to automation. Today, most tasks start off as very manual; and the company decides, well, I can't possibly continue to scale unless I find a way to do this better. But what is the case? What is the problem that we're solving? First, 82 percent, think about this number: 82 percent of enterprise IT leaders say complexity is impeding my success. I can't move as fast as I want to. That's interesting. But then it goes a step further, 55 percent say I [don't even] have an understanding of where I'm even spending dollars today, let alone the labor that goes along with those technology dollars. That's a bit of a problem. And then perhaps most shocking, in the next five years, because of generative AI, we're going to see a billion new applications that are built. There is no amount of humans that can handle this level of innovation and what is coming for us, which brings us back to automation. It's really no longer an option. This has to become the default for how companies run their technology, run their operations. Arvind's keynote centered a lot on AI, and you've seen everything that we're doing there. But another dimension to AI is to think of AI as an ingredient. Let me give you an historical example, where while AI is the spark for automation today, if you look back in time, there was some other inventions. How about the combustion engine? So, think about this, the engine was invented; and then, what were all the adaptations from that? It became automobiles, it became construction equipment, even to modern day F1. All of this was started by that simple spark, which was the combustion engine, and I think AI is that spark for automation today. Now I know there's some car classic people in the audience who loved learning on a manual transmission, but let's be honest. Once we got the automatic, it kind of expanded the market pretty dramatically in terms of who could drive a car, who could take advantage. That's the same transition that's going to start to happen in every company around the world. We can do so much more and the addressable market is so much bigger with automation. IBM is delivering the most complete set of automation capabilities in the industry, and there's not even really a close second. We have thought through every single dimension here from observability, to network, to your technology spend, to how you get insights. Every one of these pieces is critical to you completing your strategy around automation. Now this is not hypothetical. I'm going to bring you back to the 82 percent stat: 82 percent say complexity is slowing me down, I can't do everything I want to do. So, the question for every leader now is how are you going to move from being reactive and start being proactive and maybe even predictive? So, let's break it down a little bit. First is observability. Being able to have insights into how all of your technology is running, whether it's on private cloud, public cloud, an edge device, this is table stakes. It's why we've built CloudPak for AIOps. It's why we've brought Instana to market. One example is Dealerware, which is in the business of fleet management: 98 percent reduction in latency because they're using observability. You could never do this with just people. It's about giving people the automation tools so that they can do their job better. Let's go even further, resource management. So, you're probably wondering what do you mean by resource? Well, resource is all of the compute, storage, servers, network cloud environment. We've delivered Turbonomic; more recently, Turbonomic as a Service. In IBM's own implementation -- internal -- we have now automated 175,000 resourcing actions, and we did that in four months. So, automatically the infrastructure is adapting to the amount of compute and storage that's needed. What does that give you? It gives you the capacity you need when you need it and it gives you savings when you don't need it. Next piece, network. And this is the thing that I'd say a lot of people don't think about as much, but applications are really only as good as the network on which they run. And I think a great example is [Devereux] Health that is now able to detect 40 percent of issues in the network before they happen so that their end user experience never suffers at all. And it's using products like SevOne, NS1, Hybrid Cloud Mesh that we released at the end of last year. These are all technologies around network automation. All right, so I want you to hear a little bit more about automation, so I started thinking, what would be a good example of this? How about somebody that serves 115 million customers? I think most of us would love to have that many customers. Please join me in welcome me to the stage, the CEO of Verizon Consumer Group, Sampath. [ MUSIC, APPLAUSE ] Sampath, how are you doing? SAMPATH: Fabulous day. THOMAS: Oh, great to see you. So, tell us about Verizon consumers. Is my data point right, 115 million customers? SAMPATH: Dude, we have a billing relationship with every second household in the country. We send them a bill, they send us a check, every second household in the country. [ LAUGHTER ] THOMAS: That is a great business. SAMPATH: Yes. THOMAS: I love it. And so how do you spend your time? As you're thinking about, you're serving that many people, I think you also have a network to run. What are the challenges that you face? How are you thinking about generative AI automation? SAMPATH: Rob, the biggest opportunity for us tends to be in our front line teams. You know, we have hundreds of thousands of people in front line: sales, service, the technicians who come to the house and [repair stuff]. So, they're hundreds of thousands, and our main focus is reducing cognitive workload of them, because they have so much in their head: what are the right price plans, what's the latest software, which plug goes in where. And when they do that, they can't spend enough time with the customers, listening to the customers, empathizing or even selling at times. So a lot of our work is taking workload off their mind, so we have something called the Personal Research Assistant today where front line reps as well as front line sales service teams can go and look up the whole knowledge of stuff that Verizon's created over the last 20, 30 years. We have personal shopping assistants that help people shop. So that's been a huge focus for us. The second is personalization. You know, we have 115 million customers, but for me, the one customer matters the most, the segment of one, the segment of me. So, we've done a lot of work in doing personalized shopping. When you and I both go up to verizon.com, we're going to get very different things. Take the expensive plan, by the way. It's good, it's better, the better network. THOMAS: I think I am. SAMPATH: Yes. THOMAS: I think my family adopts every single option. SAMPATH: But the second thing for us is communicating with customers. People want to be pitched differently. So, we've created something called Scriptify, which is our software internally for personalized software for communication. So, the e-mail is different, the text is different, the pictures are different, now even videos are different, and we're seeing six to 10 percent better conversion rate, which is massive on a base this size. So, this is real for us. Right now, all the efforts are on customer experience. At some point, we'll transfer to cost efficiency, but I'm not there yet. THOMAS: What's amazing about the challenge you deal with is you're dealing with people in stores. When you talk about front line productivity, you're dealing with people coming in on the Web, coming through call centers. You're dealing with all these different dimensions. And behind all of this is the biggest network in the world, which is the Verizon network. So, talk to us about how do we keep the Verizon network up and running, what are you doing in terms of automation and capabilities around the network? You know, the network is a great thing. When it's on like it is right now, no one will notice it. But the minute it stops for two seconds, everyone will notice it. You know, the only time my kids come running down from their rooms is when the network goes down; and in two seconds, they're down at the dinner table. It's a great feature, by the way, we put it in our products. But look for us, when you run one of the largest networks, the first thing is visibility. Do we know what's going on in different parts of the network? It's a global network. You know, we have folks in Australia, we have folks in Asia, folks here in Europe. Visibility is an important one. Second is cyber. Cyber is such a big risk. I mean, when you have a huge percentage of the world's network flowing through our pipes, just getting ahead on pattern recognition is super important for us. And also, the network is the gateway to almost more cyber attacks that happen. They don't come in through the front door, they come in through the network, so that's been a huge place for us. But the other side is also self-optimized network. So, for example in this stage here, you know, 30 minutes before this started, there was a lot of traffic outside. So, we had more RF capacity, more power there. Now the traffic has moved in here, we've been able to shift using AI and self-optimized network the traffic closer to this place. And then let's say one site is down, we're able to move capacity and traffic to take care of that site. So, self-optimized network. And the last is for enterprise customers. You know, many in this room use Verizon, keep using them. You know, we work, I know, closely with IBM. You know, SevOne is one of our products where we get visibility into a software-defined network. So, it's a thing. THOMAS: I think it's happening right now, like you say, the way you use SevOne, I think you're the largest deployment in the world in terms of automating how the network's being used. Most people in the room don't realize it, but it impacts their day every day. Like you say, when it goes down, that's when everybody notices. SAMPATH: Yes, and that's why we want to keep it the network up every single time. You know, we want a 100 percent uptime network, and visibility is super important, and we want to get ahead of the bad guys every single time, so. THOMAS: Love it. All right, everyone. Sampath from Verizon. Thank you, Sampath. SAMPATH: Thank you so much. THOMAS: Thank you. [ MUSIC, APPLAUSE ] All right. We're just getting started. How else can we use automation? I'm going to come back to this stat I used before: 55 percent of business leaders don't really understand where money is being spent. And what is measured is ultimately optimized -- let's not forget that -- and automation is going to be the key for how we think about spend management. So, let's continue the picture. The next capability is FinOps. What is FinOps? FinOps is, do you understand how you're spending money on the cloud? Simple proposition, but a lot of people are just getting started here. With Cloudability, we can now organize billing, usage, metadata. Here's an incredible story. National Rural Electric Cooperative Association saved 30 percent on their cloud cost. And this was not like a three-year project, this is in months, saving 30 percent, and we see these kind of results all around the world around cloud optimization. Once you're doing cloud optimization, then you have to think about the bigger picture of, well, how am I managing my overall technology spend inclusive of what I do on premises or on the edge? With ApptioOne and Target Process we can give you insights into where every dollar goes and the labor associated with that technology, which is often the biggest a-ha! moment for many clients. I think CoBank is a great example. They've put ApptioOne as part of their planning process. They've now reduced their planning cycle by 75 percent because what used to be manual and spreadsheets is now just a dashboard and it comes right to them. So, when we think about these capabilities, you've got to think about, what would it mean in your company to build a FinOps practice? And I think it's often best to hear from somebody that's done that personally, so please join me in welcoming the head of data, digital and technology economics from Takeda Pharmaceuticals, Lisa Lyman. Lisa, welcome. [ MUSIC, APPLAUSE ] So, Lisa, Takeda is like a brand-new startup. You guys just got started last week, month before? LYMAN: Yes, we're a baby. We're a 243 year old global biopharmaceutical company based in Japan. THOMAS: How about that, 243? LYMAN: Yeah, we're still learning the ropes out there. [ APPLAUSE ] Yes. No, Takeda has 50,000 people across the world in 80 countries and regions. And our challenge was to empower and inspire hundreds of people across the globe, mostly technologists who've never had to worry about finance before, to think about and consciously think about how they use their technology and their costs and eliminate waste. So, we started a FinOps practice, but then we expanded it and we included software asset management, we included process improvement, and then we internalized internal consumption chargeback, technology chargeback, and built a group called the Data Digital Technology Economics Team. And I like to say that we find money under rocks, and we make the world a better place. THOMAS: Love it. Money under rocks, we could all use that. So, talk about, you mentioned this word "chargeback," which to me brings up this idea of you're dealing with different business units. Everybody wants to understand, how is my money being spent? Am I getting an impact? How do you deal with multiple business units and demonstrating impact? LYMAN: Yes, it's been a game changer for us because we've really changed the way we talk to our business units and the way we talk about technology and finance together. We started by building a taxonomy or a standard way of talking, and that way we were all in the same playing ground on finance, business, architecture, technology all speaking the same language. And then we built in transparency. So, in our FinOps organization, we were using cloudability for a standard pane of glass that everyone could see their own cost attributed to their own business unit. And we've been using ApptioOne for bottoms up budgeting and continuous forecasting. Then we layered in the automation, and we added in Turbonomic. And in the highly regulated industry like pharmaceutical is, it was very important to us to have a tool that we could push the button to automate those cost savings. And then finally, we're starting to look at our end-to-end demand fulfillment process. Takeda is insourcing a significant amount of our technology services and innovation capability centers around the globe, so we need some sort of technology to manage and view all of our resources across the world, capacity allocations, and then show back that cost to show the benefits of that insourcing. THOMAS: Love it. What a story. I think you have the best title of anybody I've met all week, technology and economics. You're bringing all these pieces together. So, take us one step further, building the FinOps practice. What would that mean? You've talked to me a lot about the role that data plays as you're thinking about that. Tell us a little bit about what it would mean to put a FinOps practice in any company. LYMAN: Yes, it takes three things: consolidate, automate and motivate. First, consolidate. It's all about the data. You need to clean it up, get it all in one place. It doesn't have to be perfect, but get it enough so that you can share it and make it understandable to different folks. Second is automate. You need the right tools. Find tools that can help you find the automations, the AI to help you find those optimization opportunities and make it clear to folks. And the third is motivate. You have to change hearts and minds. And I love talking about how we took our FinOps practice and we gamified it. We put in self-funding incentives, we put in leaderboards. And the results have been tremendous. We were able to absorb 40 percent of our 46 percent cloud growth just by doing optimizations. We've saved millions of dollars in cost savings, cost avoidance just because of awareness; and, we've reduced the unit cost in over 23 percent of the services that we support. Think about that. Sorry, Rob, software costs go up every year. We all know it, technology costs are rising. THOMAS: You're managing it. LYMAN: Yes. We were able to reduce our unit cost by increasing our adoption and then increasing awareness of technology spend and eliminating the waste. So, it's been a really great journey. THOMAS: I think your tip on gamification is an incredible one that I hope everybody takes a note on, because you've created a self-fulfilling mechanism where people are excited about it. LYMAN: Yes, and it's also self-funding. THOMAS: Self-funding, even better. All right, everyone. Lisa with Takeda Pharmaceuticals. Lisa, thank you. LYMAN: Thank you very much. [ MUSIC, APPLAUSE ] All right, we are working our way through the automation story. We have talked about observability, resource management. Then we got into the network, FinOps and everything that we're doing around technology business management. But I want to bring it back to the case I made at the start, the third point. We are on the cusp of one billion new applications that will be built on the basis of generative AI. We're going to kind of have to get ahead of that. And what if I told you that generative AI is actually going to be part of that solution? And that's why I'm pleased to introduce today, for the first time ever, IBM Concert. Powered by watsonx, IBM Concert is generative AI driven insights for your technology and operations. So, today, what as many people trying to take a look at how to solve problems, now we can do that in an automated way. And the a-ha! for us on this one, I sit in on our support calls when clients call in and with a problem, and there was always one question that came up on every call. When something goes wrong, people say, what changed? What happened? And nobody ever knows the answer to that question. So, our view was we can use generative AI to have an instant answer to that question; and in most cases, solve the problem before we even get to the point of being on the phone. So with IBM Concert, you can identify, you can predict, you can fix problems before they happen. And this is about reducing risk and compliance burdens. In IBM, as we've implemented this we've seen a 25 percent reduction in how we're having to deal with compliance problems that before were all manual. So, I like to think of this. IBM Concert, this is the nervous system for your technology and operations. We're starting with one use case around critical vulnerabilities and risk, but as you can imagine, this could go into a bunch of different areas over time as we get more experience. But I want you to see IBM Concert right now, so please join me in welcoming the Vice President of Product Development for IBM Automation, Madhu Kochar. [ MUSIC, APPLAUSE ] Madhu, welcome. KOCHAR: Rob, thank you. Thank you, Rob, and hello, everyone. I am so excited to give you the preview of IBM Concert. As Rob said, the complexity of application landscape is a major challenge. It creates operational inefficiencies, exposure to vulnerabilities, and that really, really makes companies spend more time managing their enterprise applications than optimizing them. Now remember what Rob said, one billion more coming. So, we invented a new way to see and manage your applications in a very simple way by introducing IBM Concert which is going to save time. help you solve problems faster, and make decisions even faster. Let me now demonstrate how IBM Concert puts you in control. Setup takes few minutes. You define your application. It's very simple and intuitive. No complicated integrations or processes needed. You use an API to connect into your existing applications and tool set. And moreover, you can do this across any environment, any environment without changing tools or applications. IBM Concert integrates into your existing systems using generative AI to connect your data from cloud infrastructures, your source repositories, your CI/CD pipelines or your existing observability solutions to give you that application-centric 360-degree view. Our first use case will start with risk. There are many more coming. We are delivering at a very rapid pace around, and the whole aspect of that is to provide you insights, app centric insights around cost, compliance and observability all at one place. IBM Concert just doesn't give you visibility, but it's also the total control over your technology operations. Now, let's dive deeper into the risk use case. Let's put ourselves in shoes of Joy, an SRE at a large retailer. Today, if you have to ask Joy to define a CVE risk, she would go into [my tree] or other databases, figure out the connections and dependencies and such so that it could be identified and catalogued and prioritized. And as you can see in the stack, 39 percent of the organizations clear more than half of their vulnerabilities within a week of discovery. That means Joy is always, always in a reactive state. She's already behind. Damage could already have been done. And application risk is the perfect example of an inefficient process, but now with IBM Concert the experience is going to be very different. Joy would come in, click on the risk lens. AI will compute the score for her from a risk perspective. This way it will help her to figure out what to prioritize. Joy can further use Concert to drill down into high-risk apps so she can make decisions faster and better; and moreover, it gives her a list of remediations based on the impact. With this experience, she can quickly see all of the CVE risk for her applications in her company. That's amazing. But wait, it gets even better. How? Joy can now interact with IBM Concert using a chat using natural language powered by watsonx. She could ask questions; for example, what vulnerability should I prioritize? Her questions will generate insightful analysis, visualizations and recommendations so she can take action. If she sees a dependency she does not recognize, she can further chat with Concert to get more details. A process that perhaps took days or hours can now be done in minutes and seconds; and moreover, Joy feels her company is more secure because it's less potential risk of downtime, less risk on any financial or reputational damage. Now, imagine how would it be like with IBM Concert in your team's hands. It will make them go faster. They'll be more informed. They can address the responsive issues, what's been happening, but more importantly, perhaps solve problems even before they happen. Imagine a business more resilient, enhanced by performance and availability. But most importantly, it's all about elimination of unnecessary tasks and activities so that you can actually focus your employees and your company to be more productive. IBM Concert will be generally available in June -- shout out to my development team -- and available next month. This is the time to take the cameras out and join the waitlist. You will have access to our groundbreaking technology, get trials, get webinars and get content. With that, I'm going to thank you all and say again, join and enjoy IBM Concert. Thank you. [ MUSIC, APPLAUSE ] THOMAS: All right, Thank you, Madhu. That is an incredible use case for generative AI. Some people like writing speeches in the prose of Shakespeare, we like solving real problems. We think this is an incredible example of applied AI. But I want to you hear from one of our early access clients that's been doing this with us for the last few months. We have the Chief Technology Officer from the Boston Consulting Group, Ben Connolly. Ben, welcome. [ MUSIC, APPLAUSE ] Good to see you, Ben. CONNOLLY: You, too. [ MUSIC ] THOMAS: All right, Ben. You are an innovator. You've been pushing us. Tell us a little bit about Boston Consulting Group and why this even matters for you. CONNOLLY: Sure. Well, hi, everybody. So, a lot of you may know Boston Consulting Group as the strategic consultancy that it is, of course. What you might not know, though, is that we also develop thousands of software applications and changes for our customers every single year, many of these with short development windows as well, sometimes two or three weeks. And also, over the last 12 to 18 months we've had a real turbocharged focus on the gen AI side of things, too, and so now we're really happy to say that we have over 150 gen AI-powered applications out there in our clients' hands right now. So, super exciting times for us. THOMAS: Excellent. And you're dealing with always a variety of vendors, variety of technologies, and you're also dealing with client intellectual property. CONNOLLY: Indeed, yes. THOMAS: How do these things come together? How are you able to meet that need? CONNOLLY: Great question. So, a lot of the challenges we face in bringing these to life for our clients I think everybody in this room is going to be quite familiar with. Let me just bring to life some of the nuances that affect mainly due to the nature of our work at BCG and in consulting more generally. So, starting with security, of course. As you say, Rob, we look after not only our own intellectual property, but also that of our clients. And so we have a real focus on the storage, the transit, the access controls that I'm sure we're all familiar with. And in a world with even more distributed ways of working, for us, it's not just of course at the home and in the office but very much so in transit and a lot oftentimes at client sites as well, integrating quite deeply with the ecosystems and networks of our clients as well. So, really enabling and assuring we have an always on, a constant focus and a constant assurance of compliance and risk and security is really, really important to us. And just on that point of compliance, for example, we work in multiple geographies, multiple jurisdictions, multiple frameworks within those jurisdictions as well, so it can get to be a really complicated and often moving target. And trying to identify what that real target state for compliance is can certainly add a huge amount of complexity to how we work. And probably finally and maybe the most complicated, most technical, is the sheer volume of applications and configurations that we need to interface with when connecting to client sites. This can often be in the hundreds, and really does drive that need of understanding of unfamiliar ecosystems and getting the best view of the compliant nature within it. So, a real focus on automation, on always on and continuous view of compliance, not just to give us the confidence that it's happening at the time but also involved in exactly the right part of our development cycle as well. THOMAS: Love it. So, where does this go from here? You talked about AI-powered automation. You talked about mitigating risk. What is next for you and your team? CONNOLLY: So, the real focus for us now is on, so as you said, we manage our clients' intellectual property, so we have an obligation to keep that super safe and the compliance levels that come with it. We also have an expectation from our clients to deliver at speed. So, it's something, a balance without compromise that we're constantly focused on across BCG teams. This can really introduce...that's why automation and AI is so important to us, but it really introduces a couple of things that we need to focus on. For example, from the automation side, we need to make sure we don't introduce gates. We don't want to slow down or introduce bottlenecks into our development cycles. And so we really try and drive this into the hands of our engineers, into our pipelines, automate as much as possible. And that can come across in many ways, from static code analysis at the point of commit or all the way down to dynamic testing of the applications itself at the end of the pipeline. So, really trying to get that balance right. But and the other thing we need to really focus on is driving value from this automation. So, we might have it in place, and I've certainly been guilty in my career of being reassured by the volume of the code base in our automation suite or the number of tests we have in a test suite, not necessarily always focused on the value and the outcome that these are driving. So, that's a really important concept for us to have this in the hands of engineers. And products like IBM Concert, for example, with the risk lens and the risk focus, really helping us with the data and the context that our engineers need, putting it in the hands of them, helping us discover complex ecosystems, like I say, across our vendors, but also helping us discover what compliance state those folks might be in at the same time. So, getting this information, this context into the hands of our engineers, allowing them to build automation that really adds value and focuses on the outcome that we're driving for, ultimately then allows us to free up the engineers and the rest of our teams to focus on spending more time delivering value into the hands of our clients. THOMAS: Love it. Well done. Ben Connolly, everyone. Ben, thank you. CONNOLLY: Thank you. [ MUSIC, APPLAUSE ] All right. We are in the homestretch. It is really interesting to hear how businesses are starting to push the envelope on automation. It's all about innovation. It's all about speed of delivery. Let's kind of recap back where we started. We've delivered the world's best hybrid cloud platform with Red Hat. You also know all about what we've been doing with watsonx to deliver generative AI for business. We're at this point now where all these pieces start to come together, and you can see why watsonx is at the center of this, because watsonx delivers the generative AI that you need for all these automation capabilities. But there's one other item there. We did announce our intent to acquire HashiCorp, and when I look at HashiCorp, it's the number one company for infrastructure automation, leading products like Terraform, Vault, and many others, it works on any infrastructure, any application. It is the easy button for cloud. So, it's incredibly complementary to what we're thinking about with our vision around automation. Now, I want you to all take a moment to imagine a future. You've got AI driven insights at your fingertips. Your IT operations are autonomous. Your employees are now liberated. The job just feels different. It's more innovation. It's less maintenance. It's more optimization and less firefighting. Generative AI is making this happen. It's more leading as opposed to following. You don't have to imagine anymore. The era of AI-powered automation has arrived, and IBM is the partner for that. Thank you all. Have a great week. [ APPLAUSE, MUSIC ]




ANNOUNCER: Please welcome Senior Vice President, Product Management and Growth, IBM Software, IBM, Kareem Yusuf. [ APPLAUSE ] YUSUF: How are we all doing? Wow, lots of people in here tonight, ready...or, is it this evening? Where are we in timeframe? Ready for what really is our last keynote of the day of what has been a pretty action-packed day, I'm sure you'll all agree. So, let's get into it. AI assistants have really emerged as being an essential way to improve people's productivity. But the fundamental question we should be asking ourselves is, why? From my perspective, it really begins with the fact that they speak to our...they tap into our innate desire to communicate, to converse as part of doing work. We like to ask a colleague how to help us out with stuff; and quite frankly, like to engage with systems in that same conversational style. With intelligent automations, AI assistants also help us offload repetitive tasks, enable self-service work; and quite frankly, help us link work across multi-step and multi-processes. And this value is playing out in clients as we see them today. Whether you think about it in the context of transforming business process automation -- otherwise thought of as digital labor -- in the well-established realms of customer service, or indeed, in doing development code, AI assistants are showing meaningful value in these spheres of influence. So, what have we learned from working on all these AI assistants, building them in practice in the wild? Well, the first thing I would say is that AI assistants really have to be purpose built. They have to be tailored to a given use case. It's all about enabling that assistant with the knowledge and the domain expertise to bring meaningful value to the task at hand. Generally speaking, they need to also automate some tasks, driving forward key automations -- and we talked a lot about that in the last keynote -- to actually accelerate work; and in order to do that, they need to integrate into your key operational environments such that appropriate work can be triggered and done. These lessons have shaped how we have been thinking about the product portfolio and some of the key products and new products and new product updates I'm going to take you through today. So, let's begin with watsonx Orchestrate. This is our product for basically building AI assistants based upon conversational AI and intelligent automations; and today, we have introduced a new capability called the Assistant Builder that is fundamentally about enabling you to build these assistants at speed and in a repeatable way. Let's take a look. Now. the experience begins with what you would normally expect: your basic form, create the assistant, in this case, a sales assistant; and voil! You have your assistant architecture laid out. Now, at this point in time, you can actually run this assistant and begin to converse with the default LLM; in this case, our Granite model. But what really becomes important is, how do you begin to tailor this assistant? And a clear way to begin to think about this is with obviously grounding it with more content. So, you want to provide key data that begins to give more information to the assistant about the domain area in focus. And you can begin to add in appropriate prompts that help to guide the model work, making your assistant a lot more tuned to the subject matter at hand. As I then pointed out, though, you want your assistant doing some stuff for you. And in watsonx Orchstrate, we think about this in terms of skills. We have built up a catalog of about over a thousand skills that does the work in preintegrating to target systems associated with the domain. In this case, being a sales assistant, we're going to be talking to Salesforce and invoking a skill to create a lead. But it doesn't just stop there, because obviously, as you think about building out what this assistant can do, you want to begin to string together multiple skills. You want to build this into essentially simplified and dynamic workflows that leverage all the normal primitives you would expect to allow you to affect the actual activities that you want to happen, putting the actual power in the hand of your assistant to drive meaningful work. Of course, in doing all of this then, you can begin to create multiple assistants that allow you to target very specific tasks within the enterprise; as I said, tailored to the use cases that are relevant. We've been focusing, obviously, on pre-building some critical assistants targeted to some high-value use cases. The newest one we've built? watsonx Assistant for Z. Built on watsonx Orchestrate, we have built an assistant that we've leveraged domain knowledge around Z to begin to create an environment to up-level expertise, allow new people to the platform to engage, but even more importantly, allow you to begin to capture key knowledge and best practices within the tool. You begin, as you would expect, with a typical query. And there are some folks in the room staring at that error message, they'll happen to know what it's all about. But if you didn't, watsonx Assistant for Z helps you through. Now, even more importantly, as you begin to converse in this context, it is about identifying critical skills that can help you with this work. In this particular case, how do you address things in the environment? Relevant skills are detected; and as you would expect, guidance as to the actual inputs required to trigger the skill. And where do these skills come from? They come, in this particular instance, from the Z environment that's already been maintained by customers, with our discovery agents that have been built in to look up JCL scripts and Ansible scripts, effectively import them into the environment and provide an easy template to leverage the skill as-is or readily expand it, as you saw before. You're getting my flavor of tailor-made automation and integrated into the environment that you're trying to deal with. Let's take another tour. Let's think about the world of code. Now, last year, we introduced the watsonx Code Assistant with a focus first on Ansible and then one for Z that was focused on COBOL applications. And as we built the watsonx Code Assistant for Z, we really took a full lifecycle focus to think about understanding the applications, readily explaining what they do, transforming key components of the applications into other languages, like Java in this case, and then successfully deploying them. And we have announced some major updates this today here at Think around how we've expanded the explanation capabilities. Now what we have done, brand-new, and this will be available in October this year, is built a watsonx Code Asistant for enterprise Java applications. Yes, those good old-fashioned JEE apps that we all have running on WebSphere application servers somewhere that are out there and that you want to modernize or leverage and understand in effective ways. The power of generative AI is in how it allows us to begin to work through this cycle; observe here, building and understanding by explainning the application to us. Note also how it is directly integrated into the IDE. This is important, because this is about providing the assistant in the tool of choice. Understanding the application leads you to key considerations like how might you plan an upgrade for this application, and what are the key considerations that you need to keep in mind? Issues can be readily identified by the assistant and guidance towards how you might fix them. And then that good old chestnut, let's get some testing going. AI is really good at helping us to build automatically generated test cases that match those issues and key considerations that we need to think through. And when you've happily got through all that, how about asking the assistant to quickly summarize all the key changes that have occurred within the environment so that you know all that needs to be done. Everything I've shown you as we stepped through these screens here is real code running in real life. watsonx Orchestrate with Assistant Builder available today. watsonx Assistant for Z will be available in June, and watsonx Code Aassistant for enterprise Java applications coming to you around the October timeframe. This is what I mean by building high-value assistants that meet tailored, specific work. But wait, there's more, because we have not only focused on how we build these targeted assistants, we have also looked at how we embed generative AI and assistants into applications that you already know and love to bring assistance in context. Take for instance, planning analytics. Here, as you can see, the built-in assistant begins to engage around core content, allows you to explore data that's already in planning analytics and effect changes within the UX and UI. This is assistants in context for what needs to be done. A not dissimilar pattern, we're doing the same thing with Apptio. Now, the assistant helps you to better explore key things around cost analysis and also guides you towards actions you might need to take and the ability -- and the ability -- to trigger responses and actions effectively via the link-through to automations. Lest I forget, planning analytics available in June. Apptio coming to you in the second half of this year with this embedded capabilities within. Now, you might think that all of these invocations are just what I would call user-driven, but no, you can build systems in ways that allows the conversation to be initiated by the system itself, as you might see in what we're doing here with Guardim, with alerts being surfaced by the assistant directly to the user who then can begin to engage on appropriate remediation actions. Tailored, automated and integrated into the environments that you have. And for one more, just to give you a very different flavor, assistants can be totally headless, as shown here where we have used the assistance within Maximo to automatically begin to classify problem codes within work orders. This is solving a key data quality problem and saves a lot of time in how you build out these systems or what it needs to maintain them, which obviously translates into cost savings. So, step back with me for a minute and think about what I've just shown you as we look at those quick demos. AI assistants are really, in my mind, about enabling productivity in what people need to do; and to target that correctly, you have to place it against key use cases with real understanding and ability to touch and affect the systems that matter. That's how you unlock the actual value. And so with our new Assistant Builder in WatsonX Orchestrate, or with the assistants we have pre-built for you around WatsonX Assistant for Z, or Code Assistants, or how we embed it within key applications, you're beginning to see this key flavor of what it means to do productivity at scale with AI assistants. But let's not just hear it just from me. Let's actually hear from some clients as well who have been actively deploying these technologies in production and the benefits that they've achieved. And to guide us through that conversation, I'd like you to please welcome Kelly Chambliss, our SVP of IBM Consulting Americas. Kelly. [ MUSIC, APPLAUSE ] CHAMBLISS: Thank you, Kareem. Lots of exciting news. Good afternoon, everyone...I guess we're evening, I think Kareem said. It is my sincere honor today to host a client panel where you will hear from a couple of clients that are using the technologies that Kareem just described today to deliver real business benefits to their organizations. Clients are at the heart of everything we do in IBM. One of our core values is dedication to every client's success, and we support our clients in doing that across both IBM Technology and Consulting to help clients realize productivity gains, deliver improved customer experiences, enable new business models, and even create new products. As we support our clients' path shifting from experimentation to scale with AI, we do that by helping clients determine what models to use, what technologies to use, and really fundamentally redefine how work is done. And we do that by bringing the right people, the right assets, the right skills, to make it all work. In other words, we apply deep industry, domain or process and technology skills to really embed AI into the core of business processes, into the core of business operations. We do that often by leveraging IBM Garage, a method we use in IBM Consulting that allows us to fast track innovation, realizing value three times faster than traditional approaches. And earlier this year, we launched IBM Consulting Advantage, which is a services platform that's full of assistants similar to those that Kareem described, and has built-in security and governance and allows our 160,000 consultants around the world to deliver solutions for and with clients and augment their skills, their creativity, their technical skills to deliver solutions with greater quality, consistency and speed. That is how we are putting generative AI to work within IBM Consulting, all with an unwavering focus on helping our clients achieve their business goals and establish a competitive advantage. I think most organizations today are seeking to leverage AI to establish a clear competitive advantage powered by AI. In our latest CEO study, 60 percent -- six-zero -- of CEOs believe that the winners in their industry, the companies and organizations that will develop a clear competitive advantage will be those who have the most advanced generative AI. And think about that. That's only 18 months or so after AI emerged as probably the single greatest value creation opportunity any of us will see in our lifetimes or at least careers, right? So, the next question that comes up is, okay, so much possibility, how do we make that all happen? How do we leverage AI to develop and establish and even maintain a clear competitive advantage? So, we have two leaders with us today from organizations that are ahead of that curve, who are not just experimenting and doing pilots and running proofs of concept with AI, but are actually getting real material benefits from the technology at scale today. So, join me in welcoming to the stage Kumar Gudavalli, who is head of tech strategy and chief architect at Elevance Health. Elevance Health is actually using Watson Assistant that you saw from Kareem to approach things that come up in the healthcare industry around benefits, claims and patient care. Please also welcome Gary Kotovets from Dun & Bradstreet, who is their chief data and analytics officer. Gary and Dun & Bradstreet are leveraging watsonx Orchestrate to build a new product in their portfolio using AI and some of the watsonx technology that we talked about today. So, please welcome Kumar and Gary. [ APPLAUSE, MUSIC ] Thank you for joining me today. I'm looking forward to this conversation. GUDAVALLI: Thanks for having us. CHAMBLISS: Maybe, Gary, if it's okay, we'll start with you. Procurement, I think many in the audience would see it as a process, a domain that can be slow, not always highest quality, has some risk associated with it. And I know you've been listening to a lot of your customers, what challenges did you set out to solve with this new product that I think you'll share some exciting news about here shortly. KOTOVETS: Sure. Just to give you a bit of perspective of what Dun & Bradstreet is, for whoever doesn't know, which I'm sure most of the customers in this room that use D&B data already, but our clients are some of the largest enterprises in the world, so nearly 93 percent of Fortune 500. Our data has records of more than half a billion public and private companies in the world as well as another 470 million contacts of B to B corporate officers. We solve many different use cases across many different enterprises for our clients: credit decisioning, sales and marketing as well as supply chain are some of our key areas that we focus in. Procurement is one of the key customers that we have decided to really deploy gen AI capability for. We spoke to a lot of customers, and we feel that is, to your point, Kelly, in many cases, this is an area that needs a lot of efficiency, a lot of productivity. And so imagine if there is an earthquake in China. Our customers need to quickly decide, first of all, how are my current suppliers impacted? Which supplier do I pick first, second or third to go to instead? What are my options in terms of other areas in other markets that I can go to for the same product to ensure there's the least interruption? A lot of these challenges, they're already using our data for, but it takes time. It takes time to assess, and it takes time to validate and it takes time to act. CHAMBLISS: Definitely. And as I teased earlier, we have some exciting news to share, I think, as it relates to, I mentioned earlier that AI can be used to not only drive productivity gains, efficiency gains, but also to develop new products. Do you want to share exciting news around building a new product that we will launch together? KOTOVETS: Yes, absolutely. So, I am completely excited to announce and introduce D&B Ask Procurement. It's a new software designed to empower procurement professionals with trusted data into supplier risk and insights. It's available for beta users, and everyone here, I think, can have access to it, and it's downstairs at the forum if you want to see a demo. This is a solution that combines a lot of the data and dozens and dozens of analytics that are coming out from Dun & Bradstreet as well as IBM watsonx Orchestrate to create a new experience for procurement professionals. They can ask a few questions in natural language, and it allows them to get the same data and the same information within seconds. So, as an example, and what I mentioned before in terms of needing to get to the answer quickly because of an urgent issue or because you're really just doing your day-to-day work, for whatever reason, you, as a procurement professional, have challenges. You can quickly use this capability and get access to all the data and all the insights immediately. On average from a productivity standpoint, we estimate that a procurement professional can save anywhere from 15 to 20 percent of their day-to-day work. So, this is an incredible productivity gain and efficiency gains that we hope to see across our customer base in this area. CHAMBLISS: That's a lot of time for you to do more strategic activities, right? KOTOVETS: That's right. That's right. CHAMBLISS: That's fantastic. Very exciting news. Thank you. KOTOVETS: Thank you. CHAMBLISS: Kumar, let's shift to you. As one of the largest insurance payers, and I think, if I'm right, nearly 120 million customers, Elevance Health is at the epicenter of a very complex yet very important industry to everyone, because it's all about our health and well-being. There's still some frustrating pain points, I think we would all say, across the healthcare value chain. What have you set out to try and solve with AI as it relates to providing an even better patient and customer experience? GUDAVALLI: Yes, and thank you, Kelly, for having me here. So, as you said, I think the frustration is because the environment is so complex for consumers to navigate. Right? So, from payers to providers to pharmacies to regulators, there's a lot of moving parts in the industry. So, when you look at the basic premise of experience, members have trouble navigating healthcare, you know, asking basic questions like coverage or claims, understanding how a claim is paid, why I'm billed certain amounts and things like that. So the way we took, again, our AI journey started almost four years ago. We brought in the best of our internal AI plus Watson AI together and really focused on how do we get increasingly more proactive and predictive about what customers are going through. Can we do something for the [matter] of time? And then we brought those people that needed more help into our digital experiences, which are automated with Watson. Right? And then, you know, always backed by a good human support behind the scenes, there's always a human behind to help if needed. So, and with that experience, we're able to contain about nearly 50 percent of the AI interactions without needing a human touch. That's pretty incredible if you think about that. CHAMBLISS: What percent? GUDAVALLI: Nearly 50 percent. CHAMBLISS: Five-zero, wow. GUDAVALLI: Five-zero, yes. And then if you...and we didn't quite stop there, we also looked at proactively covering service experiences. If something went wrong or somebody was dissatisfied, can you use messaging to go and engage back with them? We're doing all those things. So, again, a lot of the journey, again, there's a lot more to be done, but we're looking forward to really building on it, and with watsonx and the new possibilities, we're going to continue to build on that. CHAMBLISS: Great. I asked the question about was it 15 or 50 percent, because a lot of us assume that when you get that level of productivity gains and you're able to handle 50 percent of the customer interactions without human touch, that the customer experience would decline, right? And actually, you're seeing quite the opposite, which is, at the same time productivity and efficiency is going up, your customer scores and advocacy scores are going up as well. How have you made that happen, or what's been key to that success? GUDAVALLI: I'll call it four key things, Kelly. Like one, obviously, we made it a very cross-functional effort. Our business partners were at the table from day one. It's a very cross-functional execution teams, all setting up to the same goal, same metrics, outcome, so with an incredible clarity of what problem they're going to solve. So, it's a big one. Right? And then the second aspect is we never compromised experience or quality for efficiency. That sequence is important, right? @Forget the experience [in] quality, right, efficiency will follow. We saw that in real terms. Then the outcomes were pretty dramatic, again, 40, 50 percent containment, we had 45 million contacts. We shifted a third of them to additional channels powered by messaging. Our CSAT and [effort] scores are five to seven percent higher than our traditional channels. So, it's pretty good results that we started unlocking. And then the two other factors I'll call about is, Kareem touched on tailored AI, like which is really being purposeful, being relevant of what problem you're trying to solve; fit for purpose is what we used to call it. You know, are we broad brushing or are you really being intentional, right? And that's where the possibilities of watsonx and the power of the generative AI, I think it's...we need to be a lot more intentional, and we're looking forward to unpacking that. And finally, I want to call out some great partnership with IBM Consulting and [Shobit Larshny] and Glenn Finch and those teams have been, again, really good partners from day one. So, how do we share the values, accountability, and then do the right thing, and then the outcomes will follow. So those are some of the things we have done. CHAMBLISS: Very helpful, thank you. And I think all of us in this room feel like there's endless possibilities for AI. So, I'm curious as a last question for both of you. AI can drive productivity gains, customer experience benefits. Where do you go from here? What's next for each of your companies and organizations? Maybe, Kumar, we'll start with you. GUDAVALLI: Yes, and so the way we...I look at it is obviously, you know, healthcare in general is complex, the healthcare worker's job is very complex, as you all know. We're trying to disrupt the complexity. Right? Can we use AI to really make the job simpler, make the employees stay longer, build a career in healthcare and our front line? So, that's going to be incredibly helpful for our customers as well. Right? A happy employee is a happy customer. So, reducing job complexity in a very meaningful way. The second one is unlocking the data to drive personalization at scale. I mean, that's something easier said than done particularly in healthcare, because that's something we have to now take a new crack at that, right, and say, can we drive personalization at scale? And then he third one, obviously, all the efficiency things that we all target. Right? I mean, the way I would look at it is that are there things you can do more transformationally than incrementally? A lot of times we are doing incremental use cases, but can you take that leap and go towards transformation? Again, those are probably the three examples I would provide. And then but again, the one key thing I would say is approach it holistically is going to be the key. How do you bring everybody to...the culture, the environment, the organization, everything. CHAMBLISS: You said a couple things that really resonated with me there. Simpler is always better. GUDAVALLI: Right. CHAMBLER: I think everybody would agree with that. And very subtly, happy employees equals happy customers, right? And so putting tools in their hands and make their jobs simpler, easier will go a long ways. Thank you, that's very helpful. Gary? KOTOVETS: So, from our perspective, we think about the customer and our customer's journey as they utilize our content for some of the most critical decisions, whether it's risk or sales or marketing or supply chain management. And we always have that hat on from our customer's point of view, and really gen AI, I think, brings a very new perspective and a way to make their lives easier in the sense that we can help them derisk by helping them create, you know, provide more accurate and faster responses and more actionable responses. We can help them improve productivity and efficiencies by driving that across their organization; and most importantly, drive growth for them. As they become more productive, as they derisk, they'll see...or, will hopefully see a lot more growth and revenue. CHAMBLISS: I agree. I agree. AI will not only drive the efficiencies and productivity gains, but open up opportunities for more product, as you're doing with Ask Procurement. Great example. Well, clearly a really exciting time for all of us to be working at the intersection of business and technology. Really appreciate, Kumar and Gary, you sharing your stories today and also your trust in IBM. Thank you. GUDAVALLI: Thank you. KOTOVETS: Thank you for having us. [ APPLAUSE, MUSIC ] YUSUF: Thank you. Great job, thank you. Kelly, some amazing stories right there. CHAMBLISS: Great stories. YUSUF: So, we've got clients doing stuff, we just talked about a whole bunch of new technology, all supported by our Technology Atlas as to where we're all going. What's your big takeaway from all of this? CHAMBLISS: So much to take away, but I think, Kareem, the thing that stood out to me most is there's so many experiments, pilots, proof of concepts going on today that also in parallel companies like Elevance Health and Dun & Bradstreet are realizing business benefits, real material business benefits from AI today and so much more to follow. But I think it stood out to me that they're not just doing pilots and experimenting, they're seeing benefits at scale today. YUSUF: In other words, there's a path out of the sandbox to actually deliver value within your companies. And so everything that we're doing here, starting from going down to the technical Think forum downstairs to see all the technology live that we just spoke through, or indeed, attending more sessions tomorrow, key spotlights and the like, really are all about giving you a lot more insights into how to bring this to bear at each and every place that you are and work. So, thank you. Thank you for spending the time with us. It's been awesome. CHAMBLISS: It has been. Thank you so much, and enjoy the rest of Think. YUSUF: Cheers. [ APPLAUSE, MUSIC ]


ANNOUNCER: Please welcome Senior Vice President, Products, IBM Software, IBM, Dinesh Nirmal. NIRMAL: Good morning, Boston. [ APPLAUSE, CHEERS ] Are we ready for Day 2? [ CHEERS ] watsonx says Day 2 is a lot more fun than Day 1. But excitement is real, folks. Excitement is real. My mom calls me yesterday, she's turning 80. She goes, IBM stock went up two percent yesterday, son. I said, really? I haven't looked at it yet. She goes, yeah, you guys made some generative AI announcements that made the stock go up. The only other time she calls me is to give me parenting advice, and she uses LLMs to do that. I'm not kidding. [ LAUGHTER ] That's my mom. But it is real, it's real. If you look at the numbers, it tells you the excitement is real, because we are at the start of a technology revolution like never before that will change the way we work, we commerce and we communicate. That is real. Look at some of these numbers, right? $7 to $10 trillion added to the global GDP. Just to put that in context, that's double the GDP, yearly GDP of Germany, one-third the GDP of United States and double the size of South American GDP. That is the excitement, that is the numbers, that's the impact we are looking at. But you know what? If you look at it, 45 percent of the companies and organizations are in POC or test mode -- 45 percent. But even more staggering is the fact that only 10 percent -- 10 percent -- of the enterprises has really embarked or gone into production with generative AI. So, the excitement is real. Over the last two years, you have seen it, the amount of money, amount of focus, the governments that are going into creating models. You heard from the Saudi government. I mean, that is the kind of impact we have, but only 10 percent. Why is that? Because I want to take you through the complexity also that exists in infusing generative AI into enterprises, because unless these come have been born in the last two years, these infrastructure, the data, the applications, all that has been existing for decades. How do you infuse generative AI into it? And there are multiple professionals that have to come together and cohesively work to make it into production. So, there's app developers, there's prompt engineers, there's compliance officers. All those folks have to come together to make it happen. But look at this. You have to take models. You know what? Hugging Face has 350,000 models -- 350,000 models. Which model are you going to pick? 75,000 data sets. That's the reality. And once you take the model, how do you fine tune it using the data? And once you fine tune it, you have to deploy it. And not only that, you have to bring it into your existing infrastructure. You have to use a framework like a @Lang chain, Llama index or Haystack to build the applications; and once you build the application, how do you make sure it has the enterprise level of security? That is the challenge enterprise are grappling with. And for that, how do you simplify it? What do you need to eliminate the complexity and bring generative AI and be able to improve that 10 percent that you saw to 50 or 60 percent? What would it take? We have simplified it. You definitely need a platform. You need a platform where you can build, run and manage. That's why we announced watsonx last year, about how can you bring your data, how can you govern your models, how can you build and run. That is the most critical piece, a tightly integrated platform. That's what watsonx was built on. But there's another element, which is the culture. The grassroots. I mean, we ourselves are doing it. We are bringing generative AI into our development, engineering, to get the developers convinced and moving on using generative AI to write the 10,000 lines of code they couldn't write last year, is what we need to do. So, there has to be a grassroots efforts that needs to happen within every organization, a cohesive way to bring everybody together along with the platform. So I talked about the platform. I want to invite Ritika Gunnar, General Manager for Data and AI product management, on stage to talk about how we are building the platform, how we are differentiating and talk about some of the new announcements that's coming. Let's go rock it. Thank you. [ APPLAUSE, MUSIC ] GUNNAR: Hi, it's great to be here today. Well, last year at this conference, we introduced watsonx, which is our data and AI platform built for business with three components: watsonx.ai, which is our studio to work with foundation and machine learning models; watsonx.data, which is our open, hybrid and governed data lakehouse; and, watsonx.governance, which provides trust and transparency to manage and govern AI. It has been an incredible year. And since last year, we have completed thousands of client projects and earned hundreds of references across various industries all around the world. And we've worked with hundreds of technology and ecosystem partners, many of you here. These are just a few examples of some of those logos. Now, in the next few minutes we'll share how the watsonx platform has evolved to help our clients move from pilots to essential production-ready AI systems by focusing on accelerating how enterprises work with AI; simplifying access to data for customization of generative AI; and finally, helping enterprises govern and manage their AI. So, let's start with how we're making enterprise generative AI more accessible in watsonx.ai. Over 80 percent of organizations today are using a multi-model approach to ensure that they have the right model for the right use case. Now, as you can see from our model library here, we aim to offer the most extensive model flexibility in the market. We provide various Open Source and commercial models. We even support bringing your own model. Now, a recent update also includes Meta Llama 3, which builds on our collaboration with Meta to advance open AI. Now, we allow you to use these models either out of the box, or we help you customize these models with your data on our platform. We even help you pick the right model for your particular use case. Now, moving on, as you heard yesterday, we have now open-sourced our most advanced and our most performant Granite large language models. This includes our English and our code models. We're going to make these available in our model library in addition to the existing Japanese and multilingual models that are already available. Now, let me tell you a little bit more about these models. We train them from the ground up, working closely with our chief privacy officer and our AI Ethics Board to ensure the integrity, the trust and the governance is in at every step of our building process. Now, these models are not just trusted, they are performant. Here is a view of our state of the art code models, which consistently match the performance among popular Open Source code large language models, and they do so at almost half their size. That's pretty amazing, isn't it? Now, we're also excited to announce a partnership with Mistral AI, where we'll bring their commercial models to our watsonx.ai platform both on premises and in the IBM Cloud, joining their open-source models we already have with Mistral. This includes the versions of Mistral Large, one of the leading AI models in the market today. In addition, we're also excited to announce the immediate availability of ALLaM, an open Arabic large language model from SDAIA, which demonstrates our commitment to open innovation and AI models partnering with the Kingdom of Saudi Arabia. Now models are just a start to become a gen AI first company, true growth comes from empowering the AI application developer. And that's why today I'm excited to announce a new AI developer toolkit in private preview that simplifies building gen AI flows. This toolkit reduces the skills needed for any developer to become a gen AI developer. Our intuitive AI developer toolkit features frameworks, flow engines, templates and tools to empower developers to easily create powerful, enterprise grade applications from day one. It offers a seamless way to build, to run and to manage generative AI. Now, most common generative AI patterns involve enhancing the large language model with data and additional tools so that you can drive the right outcome. As an example, all of us know Retrieval Augmented Generation -- or, RAG, as we lovingly call it -- is an important pattern to solve for data recency and relevancy. Now, to build a RAG application, you need to access your data, you need to chunk that data, you need to vectorize it, and then you need to have similarity search somewhere embedded on your LLM. This process can be quite cumbersome, iterative and error prone in general, and so for an application developer, this can be quite cumbersome. With our AI Developer Toolkit, you can implement a RAG pattern with just a few simple lines of code, no expertise needed. You can easily even customize these using our templates that we have. Now, once deployed, you have full observability into each query execution, and you have the tooling at your disposal to investigate performance issues. You also have access to several metrics for each inference call for both performance that you see here and evaluation, that you see here. And just like that, our AI Developer Toolkit in watsonx.ai, you can build, run and manage AI applications on top of a solid enterprise grade foundation. Now, we all know that good AI begins with good data. Last year at Think, we announced watsonx.data, our fit for purpose data lakehouse that helps organizations unify and enrich their analytical data, scale their AI workloads, doing so with optimal performance. watsonx.data is comprised of commodity cloud object storage, open table formats like Apache Iceberg, and multiple query engines, including our interactive SQL query engine based on Open Source Presto, which accelerates insights by eliminating data silos and delivers optimal price performance. Since last year, we've been focused on improving data access and integrations across the portfolio. We've incorporated the Milvus vector database into watsonx.data to support those RAG use cases that you saw. We've added support for Datagate to unleash mainframe data; and finally, we added a gen AI infused semantic layer for better data discovery and analytics using natural language. Today, we're excited to announce the upcoming addition of a new query engine Presto C++ and additional query optimization capabilities within watsonx.data. Leveraging these technologies, we've been able to deliver better price performance than other leading vendors with equal query runtime at less than 60 percent of the cost. [ APPLAUSE ] It's quite fantastic. [ APPLAUSE ] Now, let's talk about how to prepare and deliver trusted data for AI. One major challenge in scaling AI is getting the right data to the right users. Well, today I'm excited to announce IBM Data Product Hub in private preview. This cutting edge solution enables seamless, safe and governed sharing of data products for AI within an organization. Our IBM Data Product Hub enables you to easily connect to a variety of different data sources to create new data products. Here you can see a variety of different assets, including what we have for our lakehouse watsonx.data. Reusable data products are also essential for AI builders, empower your data consumers to effectively discover and reuse governed data in various sources like watsonx.data for their AI and data driven use cases. Now, you can even discover and access all your curated data with detailed metrics on quality, on access and the state of your different data products. This allows to you have a holistic view of how your data products are used across your enterprise. Pretty powerful, right? Now, let's move to the need for enterprise governance. It's no surprise that most organizations are still experimenting with generative AI projects, citing risks, including bias, hallucinations and lack of trust and transparency in how decisions are being made. Another challenge is navigating the ever-changing landscape, like the recent EU AI Act -- which is the world's first comprehensive AI law -- where companies could face fines of up to 35 million euros for noncompliance. We built watsonx.governance to help our companies build responsible and transparent AI, putting governance into the heart of the gen AI cycle. And we heard you. Since its availability late last year, we've added the ability to govern your models wherever they are with flexible deployment options. We believe this is truly game changing. As an example, today we're excited to announce the integration of watsonx.governance with Amazon SageMaker. Let's take a look. In our console, you can easily map policies and metrics and other data from models that reside in Amazon SageMaker. Here you see a particular model from SageMaker and its status and characteristics in the watsonx.governance console. You can easily manage and assess your model risk. Here you can see a new use case that we're creating, and it needs evaluating for corporate risk. It's a credit card fraud transaction...or, credit card fraud use case, and you can see through the process it's been marked at a medium risk level. You can also see at the bottom here that this use case needs to adhere to the EU AI Act. So, now that we have this together, we can simply put the details and submit that for regulatory review. You can see more details here of what's being submitted; and again, you can see at the bottom here, this is for the EU AI Act. Now, take a look at your dashboard, and as you take a look at your dashboard, you can review the governance, the risk and compliance activities. You can see how they align to your organization's existing procedures. These tailored dashboards and reports and automated collaborative tools give you insights into the state of the risk across your organization, and now you can manage your models across your enterprise with that trust and transparency. I hope you share our excitement about these announcements. We're truly focused on making gen AI more scalable and more accessible for you and your organization. And with that, I'd like to turn it over to Dinesh and our clients to discuss how they're making gen AI real for the enterprise. Thank you. [ MUSIC, APPLAUSE ] NIRMAL: Wow, a lot of great announcements. I expect a call from my mother tonight, assuming the price goes up. So, look, we talked about the 10 percent. I want to bring those 10 percent enterprises, companies that are deploying generative AI at scale on stage. We have four customers, starting with Howie Liu, who is the CEO of Airtable, Mrinal, who's the CEO of Casper Labs, Elias Zamora, who is the Chief Data Officer for Sevilla FC, and we also have our VP of Engineering, Toomas, from Bolt. Please welcome all four of them. [ APPLAUSE, MUSIC ] Toomas, I'll start with you. Talk to us about Bolt. I mean, I took Bolt in Europe, so I have an experience to explain, but we'll start with, what does Bolt do? I mean, obviously it's number two in ride hailing apps, I mean, if you take DD out. So, but it's an amazing four to five terabytes of data, and Ritika talked about the data and the price performance. I mean, tell us about Bolt. ROMER: Sure thing. I guess the easiest is it's the Uber version in Europe. So, we don't operate here, so you probably haven't heard about us, but ride hailing, food delivery, car sharing, scooters, e-bikes. We operate in 45 countries, 500-plus cities. We're from a tiny country, Estonia, and yes, we have a lot of data and it's ever growing. NIRMAL: You have some Estonians here, you know, so. [ LAUGHTER ] But what data challenges do you have? Like, I mean, obviously at that scale, we talked about...I mean, obviously with watsonx.data, but what challenges do you have with data? ROMER: So, our story here is that our architecture was a very like monolithic data lake, and then we started a migration to watsonx.data. And what changed for us was that very many things we were struggling with, we just got with the platform. So, workload isolation, data governance, and I guess the most important part was just having the confidence that the watsonx.data scales really well with our growth. NIRMAL: Awesome. Awesome. Elias, we have Dr. Elias here, who is the CDO for Sevilla FC. I know how to get...trigger Elias: all I have to say is, in America we call it soccer. SILLERO: This is something that is a kind of problematic in Europe. NIRMAL: It's football. SILLERO: It is football. It is football. NIRMAL: So, I had the opportunity to be with Elias when we announced Scout Advisor. I didn't know what a tank striker was until Elias explained, it's a forward in soccer. The scout write these names as tank. What is it called? SILLERO: Tank center forward. NIRMAL: Tank center forward. So, but anyway, we won't get into that. But so tell us more. I mean, you are using watsonx and you announced Scout Advisor in Barcelona. I mean, like are you seeing the uptake with other sports clubs also now that you're a pioneer in this space? SILLERO: So, at least we make a lot of noise, a lot of noise with the use of generative AI and watsonx in the football industry. This is something very important. I will tell you something about Sevilla FC and how we are using technology. So, Sevilla FC is a football club, not a soccer club. It's a football club. NIRMAL: Got it. SILLERO: Pretty successful, 130 years old, successful in Europe. We won in the last 50 years seven Europa Leagues, it's not bad. And the key point of this success was scouting. Scouting is nothing more than identifying talent. Then it was our main key point in the success of the club in the last five years we decided that technology and data have to be also a point. When we put technology, data and scouting, appears Scout Advisor. But why appear Scout Advisor, no? Because it's very important in football, because you cannot identify just talent -- it doesn't matter the field, football also -- based on numbers. You have to identify...performance numbers are, okay, it's important, of course, but you have to identify also talent based on human opinions, expert human opinions. The human resources officers here, they know it very well. And Sevilla FC has the biggest human resources, or respected opinions in the football industry. So, [there] are a million reports of football players that potentially can be part of Sevilla FC, and we needed it to use this knowledge in order to identify talent, to identify players. There came IBM, there came Sevilla FC and the data department, and came the agreement with University of Sevilla in order to make Scout Advisor. Scout Advisor, what it does, you just query what kind of player do you want, the generative AI make it magic and understand, for example, what is the meaning of tank center forward? It is a strong guy that can play well with the head, can stop the defense. And then look in your million of reports, in which of them appear these kind of properties. So, in such a way that finally you have a number of players that can really be identified according to what you're looking for, but based on human knowledge, expert human knowledge, that is in football. But before this kind of technology, watsonx, generative AI, LLMs could not be used properly. So, we are using technology to unlock human knowledge. NIRMAL: Talk about a customer who has deployed it on watsonx and using generative AI. I mean, amazing story, amazing story. [ APPLAUSE ] And how many countries do you go after talent? SILLERO: Ah, all the main leagues, over near 40, 50 countries all over the world, all the continents. So, football is global. NIRMAL: Yes. We got it. Um... [ LAUGHTER ] Howie, before we get started, how did you come up with Airtable, the name? LIU: Well, you know, we tried to buy table.com, it was too expensive. NIRMAL: So you came up with Airtable? LIU: Airtable, yes, and never looked back. NIRMAL: So, tell the audience, what does Airtable do; and also, I mean, you're also using watsonx... LIU: Yep. NIRMAL: ...for generative AI. Tell us why and what Airtable does. LIU: For sure. So, I founded Airtable in 2012 with a very simple vision, which was empower the people closest to the work in every company to build the apps around their data, their workflows, to power their day to day. So, if you're a marketer, you can build your own app. If you're a product manager, you can build your own app to track product roadmaps, et cetera. And you know, we've successfully executed against that vision over the past more than decade. I think what's really exciting now with AI is that if you think about AI not just as technology that we consume as a single customer, but also something we can offer to our customers. So, we have 50 million apps that have been created by these citizen developers that are close to the work, over half of the Fortune 500 is paid customers, we can empower all of those people in all of those enterprises to now build with the trust, the scalability and the cost effectiveness of watsonx on top of our platform which has the agility and the rapidity of no code, right? So, we think it's more than just us as a consumer of watsonx -- which we are -- but also being able to now offer this to all of our customers, to all of our builders, it's really exciting. NIRMAL: I mean, you have built an amazing platform. I mean, it's so easy to go build apps using your platform. It's amazing. LIU: Yes. Minutes, hours, you can sign up today. NIRMAL: I did, so... LIU: Oh, cool. Did your mom get you to? NIRMAL: ...last night. Huh? LIU: Did your mom get you to? NIRMAL: No, no she hasn't. [ LAUGHTER ] You've got to convince her. [ LAUGHTER ] As long as it's free, I think she will sign up. LIU: Yes, yes. We'll give her a discount. NIRMAL: Okay. But one thing that you mentioned yesterday that kind of stood out to me, why watsonx? Why IBM? You brought the trust angle in. LIU: Yes. NIRMAL: Tell us more about the trust aspect, because I thought that was pretty phenomenal what you told me. LIU: Yes. So, I've spent a lot of the past few years transitioning from being a product-led growth company CEO to one that's very enterprise focused; and in doing so, I spent a lot of time with the enterprise C-suite of Fortune 500s and kind of gone and visited many campuses, spent time with CEOs, with CIOs, et cetera. And what I see is this massive gap, as we've talked about today. You know, there's all of this excitement, and I think it's real, it's because there's real AI value to be extracted, and then yet there's a gap in terms of what actually companies are realizing in terms of value and gains. And to me, that gap is driven by two parallel issues. One is a matter of, how do you actually go and operationalize AI? Right? I think we can help with one part of that. I think the other is trust, and there's this just this understandably massive amount of concern around the safety issues, the models, their capabilities and performance. And so I think having the trust of IBM and both the longstanding reputation that IBM has with enterprise and with trust, not to mention leading models and a really great platform story around watsonx, I think that helps to bridge a huge, huge gap in enterprise, and then we hope to complete the final mile of actually enabling these AI powered apps to be operationalized. NIRMAL: Well, thank you, great. Mrinal, to you. I mean, obviously, another success story with watsonx.gov on the governance side with blockchain, which is very unique. I mean, that's why we wanted you on stage, is because being able to bring blockchain with generative AI, use governance, an amazing story. Tell the audience about what Casper Labs does and also how you're using blockchain in this. MANOHAR: Thanks, Dinesh. Casper Labs, for those of you who don't know us, builds enterprise and industrial grade blockchain infrastructure and applications. So, you won't hear me talking about pictures of apes or canines. So, before I talk about why blockchain specifically, let's give some industry context. I don't need to convince anyone here about the unbounded enthusiasm around AI applications, but that's come with unbounded risk. The industry is in its GDPR moment, legislation is coming. Everywhere you look, there's lawsuits like New York Times suing OpenAI, for example, and AI governance is now an essential part of the AI tech stack, and it's actually going to accelerate usage and not slow it down. Blockchain brings three things to the table that a regular database doesn't. First, it's completely tamper resistant and transparent. Hundreds of validators, tens of thousands of delegators validate every single imprint, so you have an end-to-end trace that is completely tamper resistant. Second, it's fully time synced, unlike a database that desynchs when you want to maintain asset properties. And then finally, with databases, you're beholden to SQL, which is just an input/output language, whereas on blockchain you're [churring] complete, meaning sophisticated access control, sophisticated data control and security are all possible. And right now I'd love to announce in front of you guys for the first time, we're launching a product in Q3 called Prove AI, which we built in collaboration with IBM. It will set a new standard in AI governance, which will integrate seamlessly with the watsonx platform. We're mega excited about it. If you want to join us on this journey of state of the art AI governance, talk to us or our people from IBM as well. You guys have been wonderful partners. NIRMAL: Awesome. Awesome. [ APPLAUSE ] Wonderful. All right, folks. I mean, thank you for taking the time. I mean, obviously it's about scaling AI. I mean, this is all about, I mean, you are in the 10 percent. You have taken generative AI to next level. In 10 seconds, what would it take to scale? Toomas, I'll start with you? ROMER: I think it's about finding the right places first, where to use generative AI to have actually a lift in the business. So, that's where I feel the pressure, like finding those spots; and with IBM's offerings, it's so easy to experiment, so. NIRMAL: Wonderful. Thanks, Toomas. What about you, Howie? LIU: I think it's empower the builder with trust and agility. We've been running workshops with our builders, big, big companies represented coming to our workshops and learning how to prompt engineer and build with AI. And equipping them with the trust of watsonx plus the agility of our platform has actually yielded really interesting breakthrough results in the span of a day. So, trust and agility, that's where you get the value. NIRMAL: How about you, Elias? SILLERO: Locally speaking, thinking locally, I think I completely agree with Toomas is the use case. The technology is so disruptive that the use case is fundamental to know what problem do you want to solve. But thinking globally, the situation is so disruptive, so I was speaking yesterday with some people here, 100 years ago we had [XLV] conferences, they were [enseng, marikeree, quancaray, blanc], all these big scientists realizing that the quantum and the relativity has appeared, and this was going to change everything. This was the 20th century, the century of physics. I think that what we are feeling now is exactly the same, but with AI and similar technology. So, we are now realizing that the disruption was so big that we are in the AI, probably in the AI century. So, what we have in front of us, we can just guess, so probably we will live very exciting things. NIRMAL: And what about you, Mrinal? MANOHAR: I think in general people drive faster when they have seat belts on, and I think what's really going to scale the industry is building those incredible seat belts, you know, knowing you're compliant with the EU AI Act, compliant with ISO 42001 in a really transparent, rigorous manner. And it's just exciting. It gets me out of bed every day to know that in collaboration with IBM knowing we're building these seat belts, gets me up in the morning every day, gets me on stage with a broken arm. You know, couldn't ask for more. [ LAUGHTER ] NIRMAL: Well, thank you. Thank you, Elias. Thank you, Mrinal. [ APPLAUSE, MUSIC ] I mean, you heard from the 10 percent what it takes to scale gen AI in enterprises, and that's the reality. I mean, I think Elias publicity it really well. This is the century of generative AI. This is going to be called AI century. Now, what does it take, right, to really scale it? It starts all with data, and Ritika brought the data and the platform, the work that we are doing on the data side, but it's all about deployment. The last mile of that journey will only happen when you deploy it; if you don't deploy it, it doesn't matter. So, from data to deployment. But now, for you to go learn more about all the announcements that Ritika mentioned -- it's exciting, folks -- let's go to the Expo hall. Let's hear about it. Let's see it in action. That's my call to action. Thank you. [ APPLAUSE, MUSIC ]




ANNOUNCER: Please welcome Senior Vice President Chief Technology Officer, Boston Red Sox Fenway Sports Management, Brian Shield. [ APPLAUSE ] SHIELD: Hey, everyone. So, my name's Brian, and welcome to Boston. It might be your third day here, but I hope you're enjoying the city and its amenities. As mentioned, I work for the Boston Red Sox and Fenway Sports Management. And for those that are not familiar with the Red Sox, we're an original MLB team, started back in 1901. We play a few miles from here at Fenway Park, which was first opened in 1912. You know, and that would just give you perspective a little bit, that 1912 when we opened their first game was literally six days after the Titanic sank, just to give you a little perspective of timing here. If you look over my shoulder, you'll notice that Fenway Park is part of a much broader portfolio of sports franchises. You know, from RFK racing, that's in the NASCAR area, to Pittsburgh Penguins and National Hockey League, ourselves, Liverpool football, TGL or the Boston Common Golf, which is, if you're familiar with TGL, it's kind of Rory McIlroy, Tiger Woods and PGA Tour, I think will be coming to you next January, we're the Boston franchise there. NESN, our New England Sports Network, cable network and this is sort of a sister version of that in Pittsburgh, as well as, you know, real estate. We have an equity stake in PGA Tours and Fenway Sports Management, who is really kind of the management arm across this whole portfolio, if you will. And so sharing all that's important because it's important for all of you to know just that sports is complicated. I spent a lot of my career in financial services and a lot of other companies. Prior to here I worked at The Weather Channel for many years. But coming to sports, I never really had an appreciation for some of the complexity, especially the data complexity. And so when I think about this whole environment and when I work with my colleagues in technology across all these different brands, one of the things that we're trying to really accomplish is to really get our arms around sort of how do we future-proof ourselves, how do we design our systems, our data and everything else so that it is really kind of sustainable? And a big part of that, so when I think of it, I think of it really as kind of building out almost like a sports franchise as a service. How do we basically sort of take all this technology, how do we take the services and partners and things like that to create an ecosystem that really can scale and do the kinds of things and really kind of create amazing fan amenities, incredible sports analytics and other things. So, that's kind of a little bit in the back of our minds what we think about. In order to accomplish all that, we need great partners. And so today I'm going to tell you about our relationship with IBM and Wasabi and really how that partnership kind of enables this growth of baseball for the Red Sox as well as other sports really kind of within our portfolio, especially using a hybrid cloud, and obviously, the growth of generative AI and other services and things. So, as I said before, sports is complex. The other thing that's complex about sports -- and maybe not that different from some of your respective lines of business -- is that we have data scattered a lot of areas. You know, like the fundamental challenge I think as CIOs and CTOs oftentimes is, where do we position our data so that it's both cost effective as well as being highly accessible and enabling it to enable new services and business opportunities for us, especially with generative AI and things like that. And so we've developed a partnership with a company here, Wasabi. And some of you may not know those folks. Wasabi is a hot cloud storage environment that we've had a great partnership across Fenway Sports for a number of years. More recently, they acquired Curio AI. And with Curio now part of Wasabi Air, they now have the ability to be able to basically automatically generate metadata from a lot of unstructured content. So in our world, a lot of that content is video. And when I say that, let me give you an example. So in baseball, when you go to a game like at Fenway Park, we have dozens and dozens of cameras that do high-speed video analytics. Some of those cameras run at like between 300 and a thousand frames a second. That generates terabytes of data every game. That data has to be broken down. You know, so we have our own GPU farm. I think Nvidia and Intel are okay. You know, they're not a threat for our farm. But we break down that video asset so that we can then use that content to be able to kind of analyze player performance. We start to look at sort of, you know, how much torque people are putting into different portions of their body, and how they're basically responding to, you know, different throws, and are they operating at their peak performance and things like that. So, so much of that is done transparent to the average fan in the stands. But what it does mean for us is we've got data stored locally, sometimes on prem, at the edge, in private clouds, public clouds. And so the challenge for us, like many of you, is the positioning of that content so it meets our needs. So with that, you know, we built this relationship with Wasabi, we built this relationship with IBM in their hybrid cloud to be able to kind of help tell that story and be able to leverage that data quickly and efficiently. In order to kind of also do those things, we also have a lot of content within our respective lines of business. You know, and how do we run a good business? We've got thousands of devices. Within a stadium like ours, we've got camera technology, we have access control capabilities. We've got wifi and cellular services and things like that. The challenge for us is pulling all that data together in a thoughtful, meaningful manner; and with generative AI and with like IBM watsonx, we now have the ability to kind of combine those with Wasabi Air and via IBM watsonx to make really unique products, and let me give you an example of what one of those entails. So, if you look over my back here, you'll can see this is a great example. One of the things that we're now, given the fact that we've been able to sort of tag all this data, we now have the ability to be able to create compelling fan experiences and unlock the capabilities of video that for years have really been sort of platooned in just, you know, data storage buckets. So, for us, combining the hybrid cloud together with watsonx really kind of unlocks that data and gives us capabilities that we've never had, and I think it would work equally well for many of you. So with that, I very much appreciate your time. I'm going to introduce Ric Lewis next, so thank you all very much. [ APPLAUSE, MUSIC ] LEWIS: Thanks a lot, Brian. Great story there about the Boston Red Sox and how they have hybrid cloud. They have data everywhere. it influences the player performance, the coaching, the operations and certainly the fan experience, so it's a big deal. And I know Brian mentioned Wasabi, and I also know David from Wasabi, the CEO is here, so thank you very much for the partnership with the Red Sox; and obviously, with IBM. It takes a team to kind of enable a team, so we really appreciate that. So, what Brian talked about is very common. We hear these observations from all of our clients all the time, and they're really around, hey, now that AI is a big thing, it kind of highlights our hybrid cloud. So, what we're seeing is that hybrid cloud and AI, for a while, people thought of those as separate notions, but they're really highly related. They're two sides of the same coin. We really believe that, and we really believe that hybrid benefits from AI and AI benefits from hybrid. The two are hand in hand, so that's pretty important. And the reason that that happens, I believe, is because observation number two would be, it's all about the data. You heard Brian talking about data at the edge, data on prem, data in multiple clouds. It really is all about the data, so if it's all about the data, and the data is hybrid -- meaning it's distributed everywhere and growing exponentially -- and you kind of have to adopt a hybrid mindset to be able to get the most out of your AI. And that leads to the third observation, and that is, your approach matters. You can't just haphazardly kind of show up and say, oh, now AI is really important. Where's all that data? Do I have to grab it? Do I have to move it together? How do I get value from that data? And that's what the industry is really grappling with, and the reason is because the industry's been on a hybrid cloud journey. And I would say in a lot of cases it's been an accidental journey, meaning, a lot of companies said, oh, I'm all in on cloud. I'm going to pick a cloud and I'm going to run there as fast as I possibly can. And so they started on this journey, and due to economics, due to data privacy, due to workload optimization, they kind of landed on, well, now I have a workload here, a workload here, another one here, they're kind of tied together, but loosely tied together. And I think some of you have been in this industry long enough to remember when we talked about silos in data centers, now we have silos across clouds, right? Where it's like, okay, this stuff is separate from this stuff. Now, some people will tell you, okay, the answer to that is, herd all that stuff up and put it in one cloud. But you heard Brian's use case. You can't do that. It doesn't actually, it's not practical. You need the data local. So, this whole accidental kind of thing happened at the time when people are trying to get the most out of their data, get the most value out of it by using AI. And at the same time, another tectonic shift is businesses want to be less in the business of IT and more in the business of their business and focusing on that business. So, what we found is for those clients, they've been coming to us across our organization and saying, look, can you help me? I want a planned approach. And we call that approach hybrid by design versus hybrid by default, and that is where you take an intentional approach to enabling hybrid so that you can get the value from your data. So, the benefits of this hybrid design, first you can get value out of that data that's dispersed all over the place. Second, speed. You don't have to go gather up a bunch of stuff, implement, take things to a new cloud. You basically layer over platform architecture across all of your clouds and enable the capabilities that you need. And the final benefit is trust. It's a trusted, holistic approach to security across all of those premises, et cetera. And the good news about this approach, the hybrid by design approach, is it doesn't require scrapping what you have and starting over. It basically requires making an intentional set of choices and then choosing an IT architecture that enables you. So, we're going to talk about, how do I go do that? You say, hey, this can be done and I don't have to scrap things. So, the first thing, the first key element is to get data focused. That means that you bring AI to the data rather than trying to herd up a bunch of data and get it all parsed correctly and bring that to some specific spot to do AI. It's a very important step. The second thing is you become AI infused. That means you do steps like modernization of code. You do steps like you enable full stack innovation across your entire set of assets, whether they're on prem or in clouds. And then the final thing is you've got to get platform oriented, and that's kind of those layers that you saw in that prior chart. What does that really mean about platform oriented? That means choosing platforms that operate well in a hybrid cloud environment -- platforms like OpenShift, platforms like watsonx, platforms like Concert, which allows you to do measurement across those. You can tell IBM is really in the business of creating those platforms to enable hybrid multicloud so you don't have to scrap things, you can take it as you have it and get value from that data. So, why IBM, right? One is what I just told you, is we're in that business. We're creating those platforms. But there's a lot more there. We're continuously innovating across the stack. And what I mean by across the stack, I mean from the bottom to the top, from inferencing chips to inferencing technology to models, you hear about watsonx, to tools to help automate all those things. We're continuously innovating, and we have features in Z, Power, storage, our software from Red Hat. You're hearing all about that here as you're at Think. The second thing is we have the tool box to make it happen. We have the IP that we can either give to you or that our consulting team can help coach you on how to implement it. And you're hearing all about that tool box, it's all the products that I talked about a little bit ago. And then the last thing is the talent and expertise to get there. We can either help you with our teams and our technology -- we try to make it as easy as we possibly can -- but we also have a large set of consultants that know exactly how to do this. They know how to do data transformation. They also know how to do all of the things that you need to do to enable these platform oriented architectures, and Mohamad is going to be out in a little bit to talk about that. But the final thing that's super important is partnerships. That's why I recognize Wasabi and Red Sox, it really takes a client and a vendor partnership to kind of pull all of this together. And to share a couple more examples of how we're succeeding in hybrid cloud, I'd like to bring out my next guests. The first is the data evangelist at Swiss Mobiliar, which is Thomas Baumann; and the CTO at Broadridge Financial Solutions, Tyler Derr. [ MUSIC APPLAUSE ] Okay. So, Thomas, why don't you tell us a little bit about Swiss Mobiliar and what's going on there and how things are changing in the insurance industry. BAUMANN: Okay, thank you. It's great to be here. And three things, first of all, from the Swiss Alps, [still capped], there's a lot of snow. [ LAUGHTER ] So, Swiss Mobiliar is the oldest insurance company in Switzerland. We were founded in 1826, which makes close to 200 years ago. And since the first day until today, we have been number one in household insurances in Switzerland. It's quite easy to become number one if there are no other competitors out there, but it's quite hard to stay there for 200 years -- and there are basically two reasons we made that. First of all, we're very close to our customers. We're very authentic. We operate just in Switzerland and a few miles around. And the second one is we use analytics from the first day on. Predictive models, predictions is our DNA. So, we already use the expert systems during the first wave of AI in the 1980s, we are also among the first ones using it these days. So, my role as data evangelist, I create value from data. And how do I do that? I gather my specialists like database administrators, data architects, data engineers, data scientists together, and focus them on the business project. They're all convinced that they create value from data, but they have different perspectives. If I ask a data scientist what do you understand by creating value from data, he will answer, I can enhance your accuracy for a project from 91.5 to 91.55 percent within the next three weeks. But maybe the business objective is going as fast to the market as possible. So, sometimes those specialists, they are focused for the sake of technology, for the sake of IT and not always for the sake of business. So, that's my role in allowing them to get those business results. We have achieved some major IT projects based on AI: fraud detection, churn prevention, chatting with the service desk, or even image-based claim estimation based on images of crashed cars. But today I'm not talking about these large projects with a lot of data science involvement. Today I'm talking about the data, our crown jewel of data that's sitting in our databases that's encrypted where every access to it is monitored. We don't want to bring this data, lift and shift it to another platform. We don't want to bring data to the algorithms. Now we do it vice-versa, we bring the algorithms to the data. There's a new feature in DB2 called SQL Data Insights -- which we basically call frictionless AI -- which allows within the database make queries AI enabled. So, we can use AI directly to access data in the databases. We don't need data engineers. We don't need data architects. We don't need...we do not even need database administrators. It's just a development team that knows the tables best that can define and enable AI. So, now we are able to come from idea to production within four hours. LEWIS: Fantastic. So...yes, please. Yes. [ APPLAUSE ] I love "frictionless AI," I'm going to steal that. I'm sorry, I've got to use that a little bit more. But you have a demo to show us? Let's take a look. BAUMANN: One of our first use cases was predicting the success probability of an insurance quote. So, if I'm a sales rep, I have a particular quote, a particular customer, and I want to know how large is the chance that this quote will be signed. So, the first of these four hours is I get to collect the data from different places within our DB2 databases, but I don't have to move data from one place to another. Just can define the view, and then our development team can say, enable that view for AI, which takes maybe two and a half hours training in the background of these models. So, numbers appear here. I think we've already talked about, those four hours from ideation to production. The other numbers are just highlighting that the models created are comparable to typical AI use cases on other platforms. So, now let's start this demo. First thing is gathering the data together, then enabling it for AI. That takes maybe 10 minutes for each million of data. After that, you can just send your SQL statements, give me all the similar customers compared to the customers I have. And the result is all the similar quotes sorted by descending order of similarity to the quote I have. So, we calculate the average success factor of the top, I guess, 43 quotes in this case, and this is the probability of success. If you have now our current quote in mind, we see 17 percent success factor, not very promising business, so maybe I should reduce deductibles a little bit, enhance the discount rate and recalculate it again. And I ended with 73 percent. With this, I can go to the customer. Having this in production now for about one year, we could...we measured the overall conversion rate at Swiss Mobiliar, it went up from 40 to 44 percent, which is a 10 percent increase in sales, all done with these models, four hours to production, no data scientist involvement, frictionless AI, and more time for skiing for me. [ LAUGHTER ] LEWIS: Fantastic. Fantastic. All right. [ APPLAUSE ] Tyler, I don't know if you're a skier or not, but we don't have time to talk about it, so let's go on and talk a little bit about Broadridge. What do you do there, and tell us a little bit about your hybrid cloud journey. DERR: Great. Thanks, Ric. I'm Tyler Derr, I'm the Chief Technology Officer at Broadridge Financial Solutions. We provide capital markets, wealth, asset management, regulatory compliance and government regulation solutions to the market. It's around corporate governance, and so a lot of the solutions we provide are industry plumbing that really needs to happen, non-differentiated applications that need to run at high scale multi-tenancy with features. And so this entire conversation really for me today is talking about opening up the asset that most of you may still classify as what you call a mainframe. It's time to open up the Z. This is what it's about. This hybrid cloud journey is also about the way that we're viewing using the technologies of Code Assistant with Watson, so we can do things you traditionally think about, like how do you code in a language other than COBOL. And that's interesting, but we're also trying to break down the value that we believe that we can get out of these types of applications and really viewing that asset very differently in our environment. And a keyword I keep using is "unlocking it." That unlock, really from our perspective, I'll give you a couple of personas. One is around Net Promoter Score. Your front line agents that are taking calls in, if you run a large technology stack like we do, you've got inbound calls for things that may not be going well. To have front line agents be able to look at a code explained to see in English what is happening in real time with an accident, being able to triage that and get it to tier 3, we're talking about those types of use cases in addition to the traditional use cases to get value out of code explained right now and that we can increase Net Promoter Scores. And trying to make sure that we've got good alignment between the business value and the technology that we're exhausting. That's a use case. And then on the code explain, the thing that I would mainly tell you that we found as a benefit to us is not only looking at modernizing that and of being part of your modernization journey to say, what are the business features that I want to carry forward either in a new language or a similar language, but easier to maintain, it's also about bringing your engineering staff along with you. You may have legacy -- and you can use that term "code" -- that can be well explained now that you can bring an entirely new set of engineers along with you in the journey. They're going to know what the code does. They're going to understand in case you want to modernize it. And it really opens up that, I would say the Z environment, to be very different than what we viewed it before. My entire call to action is this. If you either work in an IT organization and you have people counseling you about the value of this, unlock it. It's really the time to do it, and it becomes part of your strategy. LEWIS: Fantastic. I would like to thank my guests. Well said. Well said. [ APPLAUSE ] All right. Thank you, guys. Thank you, guys. All right, so thank you, both. Obviously we have clients all over the place that are getting value out of their data in a hybrid cloud environment. They're not buying the shtick that, oh, well, the only way to get value is to move it all to one place and then work on it. They're also doing things like code modernization to make sure that exist in a modern environment, a platform-oriented environment that's data focused. So, I mentioned the skill set that it takes to do this, and I mentioned how IBM Consulting can help you. I'd like to invite out Mohamad Ali and his guest from SAP, Byron Schmidt, to talk more about this and to wrap up for us. Come on out. [ MUSIC, APPLAUSE ] ALI: Thank you, Ric. Great session. Great session. [ MUSIC ] Well, Byron, thank you for coming all the way from Calgary, Canada. SCHMIDT: Thanks, Mohamad. ALI: Anybody here from Canada? [ CHEERS ] All right. Let's give it up for Canada! [ APPLAUSE ] SCHMIDT: Thanks, Mohamad. This is a full circle moment for me. I started my career with IBM Consulting, and I'm proud to be here as part of SAP today and representing this 50-year partnership. ALI: That's fantastic, 50 years. I've been married for 30 years, and that feels like a long time. [ LAUGHTER ] Anybody here been married for 50? That's us. Well, today I want to spend a bit of time talking through how companies are navigating their cloud modernization efforts. We'll talk through some of the challenges we're seeing from clients, offerings in place to help them get the cloud securely, and how we're working together to use AI and automation to accelerate these efforts. SCHMIDT: Looking forward to the discussion. ALI: Great. So, let's dig in. You spend your days with some of the world's largest enterprises. What are you hearing from them when it comes to being proactive about embracing the cloud, and what are they saying to you as the why they are embracing the cloud; and, what are they looking for from companies like SAP and IBM? SCHMIDT: What we're hearing from every business leader around the globe is the desire to be future-proof, to become more flexible and resilient so they can bounce back faster to unforeseen challenges. At SAP, we believe modernizing the right workloads for the cloud is the first step and the best opportunity to become future-proof. Progress towards these modernization efforts tend to be slow and complex. For our clients, SAP is the heart of their business system. They trust us with their most critical data, so complexity isn't just about managing the IT architecture. It sparks questions like how can we maintain business continuity, keep data secure and ensure minimum to no downtime during the modernization efforts? They need the resources and expertise of our partnership to help them orchestrate these transformations. It's why we are seeing our client base for RISE continue to grow and expand. We're bringing them the agility they need to seamlessly modernize and doing it at the desired pace and phasing in a reliable, secure and compliant manner. ALI: That's right, reliable and secure. I have a very good friend, a college friend, and he's an SAP engineer. And I say to him, why do you love SAP so much? You know what he says to me? He says, it just works, reliable and secure. So, you know, we've seen a lot of success together when it comes to helping clients modernize on any cloud. BCG, for example, moving to multi-tenant RISE; other RISE clients such as Lazard, Hubble, and Diageo. What are you seeing in terms of why they're selecting RISE, and how is AI playing a role in this? SCHMIDT: AI is continuing it to grow as a key leader in the adoption of RISE. Our work at Synovus, a leading integrated energy company, saw the track record and capability of our team as a key factor in their decision to entrust us with their critical transformation. At Hubble, where the complexities we discussed earlier are prevalent -- different systems, different providers, dated architectures -- we were selected to help modernize their operations. At Diageo, a leading brand known for Guinness, Smirnoff, Johnny Walker... ALI: Guinness and Johnny Walker? Did anybody imbibe in any of these last night? You're in Boston, it's Irish, Guinness. All right. SCHMIDT: Well, we announced... ALI: Let's keep going, let's keep going. SCHMIDT: We announced the adoption of RISE with the support of IBM Consulting to deliver the transformation across 180 countries. Regardless of a company's size or stage, they're dealing with immense demands and pressure and a growing talent shortage across all levels of the stack. It's this conversation around AI is why it's so pervasive. ALI: That's right. You know, gone are the days when companies can invest years in planning and executing a cloud transformation project. They need to do it quicker and accelerate time to value. That's where IBM Consulting comes in. We can now build complete SAP RISE landing zones in less than two weeks, this used to take us six to seven months. And we can implement and deploy RISE business solutions in six to nine months, which used to take us 18 to 24 months. We're bringing the right talent, but we're also maximizing the productivity of this talent. I'm extraordinarily impassioned about this topic, because we're not just doing it for our SAP engineers, we're doing it for all 160,000 consultants that we have. And I want to show you how we're doing this, because it's new, and in some ways, it's remarkable. I'll show you some of the results in a minute. So, we recently rolled out IBM Consulting Advantage. It is software, software from IBM Consulting that we use internally. It's a workbench for our 160,000 consultants to transform how they deliver. It enables more repeatability, consistency and speed to client delivery. So, how are we doing this? We're bringing together three things -- AI assistants, assets and methods -- and it's powered by a wide range of language models, including watsonx; and below that, it runs on a hybrid cloud environment. And what it allows us to do is when we're doing an SAP deployment is use gen AI tools and assets and assistants for everything from requirements to coding to testing to production, the entire lifecycle, and that is allowing us to see remarkable productivity gains. And it just doesn't save you cost of implementation, but you're using less talent, and you all know that SAP resources are in short supply these days. So, an example is at Water Corp. in Australia where we utilized this approach with watsonx Code Assistant to move 100 SAP systems to AWS, and the results were just remarkable: 30 percent reduction in development costs. Who here wants to do their SAP project 30 percent cheaper? Anybody? Nobody? [ APPLAUSE ] Okay. All right. Thank you. And then on an ongoing basis, the client is seeing 40 percent reduction in cloud operating costs and 1,500 hours reduction in manual labor costs per year. So, with AI and cloud and hybrid cloud, together we're seeing drastic benefits to our cloud migration. So, Byron, how do you see the convergence of cloud and AI playing out long term for businesses? SCHMIDT: Well, we're at a pivotal moment, and the latest announcements related to IBM Consulting Advantage will play a critical role in delivering the workload ahead. We have the power of cloud helping companies better access and share data, and gen AI to help them automate, analyze, and drive value from these insights. Together we recently launched our Value Generation Partnership, which allows us to take our collaboration to the next level, delivering greater client productivity and innovation with new gen AI capabilities and industry cloud solutions. The expanded partnership will help more of our joint clients innovate through cloud, data and business AI to grow and transform their businesses. I speak with CIOs all the time about how to make the most of their cloud investments, and whether it's cloud or gen AI, my message is simple. Now more than ever, experience and integration across your systems matters. You need partners like SAP and IBM to unlock the power of responsible, reliable and relevant AI for business. We're already doing the work to use it and embed AI, and we're seeing it pay off. ALI: That's great, Byron. I can see massive opportunities as we build IBM Consulting Advantage with more SAP-specific AI assets and assistants, and thank you for your recent partnership in building and expanding that. That was, we announced a few weeks ago. This will enable our teams together to deliver greater value to our clients with less resources faster. Thank you. SCHMIDT: Thanks, Mohamad. [ APPLAUSE ] ALI: So, what's the big takeaway from this discussion? It's that there is a symbiotic relationship between hybrid cloud and AI. As Ric said, two sides of the same coin: a secure hybrid cloud approach is needed to scale the adoption of AI, and AI can accelerate the execution of hybrid cloud. Our clients and partners demonstrated the impact that embracing hybrid cloud and AI together can have; and we at IBM bring the infrastructure, consulting expertise and partnerships to help accelerate that journey. We invite you to please head over to the Expo floor to see our technologies in action, including the IBM Consulting Advantage booth, and join us for upcoming spotlight sessions. Thank you. [ APPLAUSE, MUSIC ]


ANNOUNCER: Please welcome Senior Vice President and Director of Research IBM, Dr. Dario Gil. [ APPLAUSE ] GIL: All right. It is so great to be with all of you, and I remember being on stage two years ago also closing the Think Conference. And I remember talking about the future of AI being foundation models. And I remember at the time maybe that sounded a little bit abstract and theoretical. This was a pre GPT days. And boy, has that happened, huh? So, as that evolution of technology happened, we have seen a proliferation of foundation models of every possible size and capability, and it feels to some degree that the task is continuously keeping an eye on them, evaluating them, figuring out what use cases to leverage them for. But today, I'm going to challenge all of you to go on a slightly different mission. But before I give you that mission, I'm going to focus on what is truly going on and what is essential in this modern-day AI revolution, and that has to do with the power of data representations and the power of being able to encode incredible amounts of information of every possible form inside this new, incredibly capable representation that are foundation models. And really to understand how profound this is, I would like to briefly touch and go back to the origin of our digital world, an origin that was understood and conceptualized almost 350 years ago by Leibniz. Leibniz already then understood that you could take the information that was available around us in the form of language or mathematics or you name it and be able to encode it in a binary representation. To create everything, one thing is sufficient, he said. He already knew the value and the power of representing information differently; in fact, the last number of decades have seen tremendous amount of value creation and of business transformation driven by the evolution of data representations. As an example, we could encode data in a relational database. When relational databases were invented and created, it allowed a different way to organize and connect data that we couldn't do before, and also it had a very profound impact not just on technology providers like IBM, Oracle and many others, but on enterprises. All of a sudden, we could do payroll and transaction processing and so many other core processes differently. You could take the data and you could encode it differently through a graph. You could have nodes and edges and traverse that graph, and that representation turns out to be very important if you're in the business of doing things like Internet search or social media and doing graphs in terms of connecting people and groups. For the more geeky among you, you could take temporal data as a signal from an EKG, as an example, and you could transform it through a Fourier transform to a frequency representation, and all of a sudden you can do signal processing. Well, what is going on right now, it is actually the ability to have foundation models that can take training data and be able to represent it inside these models. So, let's explore briefly how when we create a model we take the training data and we break it down into smaller chunks that we call tokens. Now, a token can be a word or a fragment of a word, and this process creates trillions of these tokens. We convert each of those tokens into a vector, and a vector is just a collection of numbers, and we use vectors to represent the tokens in a form a neural network can understand. Here, we're talking as in words, but we could do the same thing for code or images or really any other kind of data. After the tokens are converted to vectors, they pass through the layers of the neural network and we apply a series of mathematical operations that are mostly made up of matrix multiplications and a few other simple operations, but they are done at a massive scale. And as we progress through the network, it can combine and recombine information across the sequence of tokens. We can even combine information from different modalities in the same model. And during the training, what we do is we adjust the network parameters so that it gets better and better at representing the sequences of tokens; and as it goes through this process, it learns more and more of the structure of the data, its nuance and the knowledge contained in the data. So, it's not really magic, it's just math and human ingenuity and a lot of computing power. Now the power of these new representation where we encode it inside foundation models derives its capability from its scale -- just the sheer amount of data that we can bring in -- from its connectivity and from its multimodality. This connectivity is very important, because we are taking these wide, disparate kinds of data, and by being inside the neural network and the structure, we are establishing semantic connections of that data once it is expressed inside it. And I will make an observation now and remark on a contrast that while we have witnessed over the last couple of years the fact that we can literally take all the public data that is available in the world and put it inside a foundation model, for the sake of argument, let me say a hundred percent of that kind of data can make its way into a foundation model. Let me contrast it that in an enterprise, what percentage of enterprise data is inside foundation models? I would say tiny, not even one percent. So, this is an interesting contrast: all the public data has made its way in there, none of the enterprise data has made its way there. So, I want to give you all a mission -- a different way to look at AI -- and this mission is that the task ahead is to go together on a journey to represent enterprise data with foundation models to unlock its value. How many of you would you say that your data is one of your most valuable assets inside your business? I bet all of you. So, the task is not about evaluating models, the task is to figure out how to progressively, safely, securely and cost-effectively bring more and more of your enterprise data inside this new representation to create a massive amount of value. You'll recall that last year, I was talking about don't be an AI user, be an AI value creator. This is the story of being an AI value creator with data. So, the question is how? How should you do it? So, we're going to walk through three steps to go through this process of going from the zero percent to a large percentage of your enterprise data represented. The first step is you have to start from a trusted base model, because we're going to add the data to it, so we've got to know what is in it and how it works. Once we have a base, we've got to have a process to representing and encoding enterprise data in a systematic way, and we're going to show you how to do that. And dafter we do that, we have to deploy and scale and create value with your AI -- your AI. Why should you choose a trusted-based model? Let me just give you an analogy for a minute. Imagine that I give you a vessel. The vessel is going to be important, because this is where we're going to add the data. But in this analogy, that vessel at the beginning looks opaque; and by the way, the vessel has some liquid inside, somebody else's data, some public data. And now we're going to add our data or liquid to it. It's going to get mixed. You don't know what is inside. You're going to shake it and you're going to drink it. Probably not. That doesn't feel good. So, in this world, the vessel needs to be glass: you need to be able to see inside. You need to know whether it has water or ice or whatever are the right things in there, so that when you put your ingredients inside, you know what happens. So, you need a base model that has transparency and you know what its contents are, what data was used, the methodology that was used, so that when you add and mix it, you do it safely and safely. So, these base models need to have performance, that's undebatable. It needs to have transparency. It needs to give you broad commercial rights. In the end, remember, this story is not a story about providers or models, it is a story about your data. You need to have the rights, so that when you encode your information in it, you have full freedom of action to do what you need to do for your business. And also, because you're consuming something that has data from the outside world and capability, it should be indemnified. You should feel safe that you can operate in that fashion. So, this step one of starting from on a trusted base model is the reason we have built Granite, and it is the reason why we have open-sourced the Granite family of models. And this is really important, and we're very proud of this work, and I want to show you what we have created and why it matters so much for the world of enterprises. Today, we have already released 18 models from the Granite Series. You can expect more and more capability to come. But they come, there are models for coding, for helping you with software, there are models for time series. There are models for language, and there are geospatial models. And you see them here listed. So, I want to give you a few examples quickly of what they are and their performance and why they matter. So, let me begin with the code models, because we're very excited about them. I mean, code is becoming more and more the lingua franca of business, so the ability to deal with code, to write code, to debug code, to enhance the productivity of all the developers that are inside our enterprises is tremendously powerful and important. So, I want to reference the performance data that I'm going to show you. On the code models we have released a three billion parameter model, an eight billion parameter model, a 20 billion parameter model and a 34 billion parameter model. I'm going to pick the eight billion, because it is increasingly becoming almost like the workhorse of the industry. It has the right sort of scale and performance and cost; and as a consequence of this, everybody who provides kind of models has this size. So, I'm going to show you a comparison for those in a second. We trained this model with 116 programming languages. It has 4.5 trillion tokens in it. And you see here an example of some of the distribution of the programming languages with which we train the model. And it does all the things you expect, which by the way, this is almost like science fiction four years ago. I mean, it generates code, it translates code, it fixes code, it documents code, it explains code. And interestingly, it's really good at reasoning, too, so you can give it all these sort of like, you know, reasoning puzzles and so on, and it helps like think through and sort them out. But I just want to highlight a moment that sometimes we have to, like, smell the flowers. What was happening a few years back on the dream of having computers to help us with code writing and so on was almost like a dream, and look at the progress that has been made. It is truly impressive the capability of today's foundation models and generative AI. So, it does all of those things, 116 programming languages. Let me show you the performance here compared against Google's Gemma model, Llama 3 model, Mistral as well; and you see in blue on the right hand side, the IBM Granite eight billion model. Simply put, it's the highest performing model in the world for writing code for these eight billion parameters. I'm really proud of the team who has done it, because the characteristic of this is to build the highest performing family of models in the world, but to deliver it in a way that has all of those transparency characteristics and freedom for all of you to create value on top. So, these models are also special in that they're released under an Apache 2 license that gives you the maximum level of freedom of action and rights to do that. On this category, only Mistral and IBM Granite of what you're seeing here are released under an Apache 2 license. By the way, if you want to learn a lot more about this, the team has published a paper that I referenced here, The Granite Code Models, a Family of Open Foundation Models for Code Intelligence, in which you will go into gory detail of all possible benchmarks, comparisons across all different modalities, how they were trained and all the effort that went behind it. Now, we have also created a Granite time series model, and I'm going to show you the model making predictions of energy demand in Spain, in this case. So, what you can see here is that the Granite zero shot prediction in the yellow line comes closer to the actual energy demand than the statistical benchmark. Then we fine tune the model to capture correlations with weather and make the prediction on the fine tuned model and it does even better. Our Granite time series models outperform several of the most popular state of the art time series models out there. It has a lower average error with dramatic improvements in model size. And also, in another modality, and this one in collaboration with NASA, we created a geospatial foundation model called Prithvi. And we fine tuned versions of that model, and we're going to be adding that to the Granite family of models. Now, this is one of the new fine-tuned Granite weather and climate models doing downscaling of precipitation projections in Europe for the year 2095 from 150 kilometers to 12 and a half kilometer resolution, so that's 12 times higher resolution of climate projections. And our model can capture both regional and local extremes and impacts, and is the first foundation model for weather and climate to scale to global and regional areas. And these models, as I mentioned, they're available with this geospatial model that I just shared with you soon to come in Hugging Face under IBM Granite. And here they are. Okay. So, step one, the vessel. The vessel with the right properties, performance, transparency, freedom of action, indemnification. The next step now is, how are we going to take our enterprise data and add it into the vessel, add it into the foundation model; and for that, we need to create a new representation of the data. Okay, so what...before we talk about our invention and our advancements, let's talk about how we do it today, how we all do it today. So, let's say you take a large language model and foundation model and you want it to interact with your data, what can you do? One pattern is RAG, Retrieval Augmented Generation. So, what happens when you do RAG? What happens is you take documents, as an example, that are your documents and you vectorize them, so you put them inside a vector database. And what happens is that that representation interacts with the foundation model in such a way that you ground the answers that the model generates in a way that is more tailored to your documents, to your need. That's what RAG does. Notice a couple of things. You're not improving the model. None of your data is going inside the model, the data is staying outside. And are you really adding long-term value to your journey of encoding the data in a new representation? Not really, but it's a very useful pattern. So, the model doesn't improve, but it's useful application to specialize the answer that the model can create based on your documents. Now, the second thing that you can do is you can fine tune a model, so you take a base model, and now you use some of the data that you may have to actually alter some of the model weights that are present in the neural network to specialize it. So, what you end up is with a copy of the model, and you end up with a copy of the model that is specialized for a particular use case, but you pay a price in that you lose some of the generality that was present in the model. So, if you have a different use case, you take another copy, you fine tune it and you use it for that use case. So, you can imagine that as you have more and more use cases, you end up with more and more models, and then you have to manage, and you have the disadvantage that you can sort of not merge those fine-tuned models for a common share value representation. So, that's what we have today. They're both important. We all use them all the time, right? And it's the only ways that we had so far of engaging these foundation models with enterprise data if you are not a model producer yourself, right? Because there you have a third tool, which is you can go from scratch and pre-train a model that does what you want -- but the majority of folks don't engage in pre-training from scratch. So, we've been thinking very hard about this problem, and our team here in Research invented a new methodology, and the new methodology allows us to actually make these foundation models truly an open collaborative endeavor. And it does so by enabling them to learn a bit more like humans do, meaning that we can incrementally add to the foundation model knowledge and skills to progressively and steadily improve the capability and the performance of the model. So, you can actually engage in incremental skill teaching of those capabilities, and I want to unpack for you how that works and the business impact that it has. So, this new capability is called InstructLab, and InstructLab is, at its core it begins by having a taxonomy that represents the foundational skills that is there. So, let me, for the avoidance of any confusion, what we mean by a taxonomy in this case. So, a taxonomy in this case is represented by a tree, where at the top of the taxonomy, you have this Granite model. And for example, one of the things that we want our model to do is to be able to write, right? Great. So, it needs to have compositional skills, so those compositional skills that involve writing, there are different types of writing that you can do. So, one example of writing is freeform writing; and within freeform writing, there is prose, you can also write in verse. But in this case, you're doing prose. And one example of writing in prose in freeform is writing an e-mail. And you know, in this end of the branch that we're traversing here, I give you an example of what an e-mail looks like. Okay? So, you create this taxonomy, and we've created them, also available, but it's, you know, I assure you that after a few weeks of doing this you end up with taxonomies that describe a ton of what you have and what you do. Okay. So, you begin with that, and that gives you a base capability of what the model can do. So, now let's say that you decided to use an LLM to help you automate some of your business processes. That's our use case. And the base model that you chose was probably very well trained on general tasks that were related to your business, and that's why you chose it. However, it doesn't necessarily know about the specific use case for which you want to deploy this model. So, to give it those capabilities, you must tune the model with specific domain knowledge and skills with InstructLab. So, let's see how we add a new skill and knowledge to the Granite base model. So, we start with this taxonomy that I described that includes all the skills and knowledge represented in the model, and here we create a new leaf node and attach a couple of examples of the missing skill that we want to teach the model. That is, we want to ask the model...what we want to ask the model and we want the response to get back. And we give it three examples here, and we submit this request for this skill to be added to the model. Now, this request is then reviewed by your team of experts for approval; and after you approve it, the examples are sent back to the InstructLab back end. Now, we can also add new knowledge to the model in the form of documents or manuals or books or other high-quality knowledge sources. And to get many more examples similar to the ones that we submitted, we use a teacher model to generate a large collection of new examples. We also generate a large data set of synthetic questions and answers that are grounded on your documents to teach your private Granite model about your domain. And we use the new synthetic data to tune the model first with the knowledge data and then with the skills. And with your new knowledge and skills, the model just got better for your specific needs without losing the generality that makes large language models special. We do another quality evaluation on the resulting model, and if everything checks out, the model is now ready for you to use. And because we're not training the model completely from scratch, this can happen very quickly with a small number of GPUs, basically all just within one day. And here, everything is transparent and fully under your control. So, let me give you an example of our own experience of this journey with the methodology change. Remember, I told you that we created a family of code models, and one of the code models that we had generated was a 20 billion parameter model. And we had trained it on a wide variety of languages, but one of the languages that it didn't know very well was COBOL. And we had made a decision inside IBM that we were going to create a Code Assistant called IBM watsonx Code Assistant for Z to help with application modernization efforts for the Z platform, et cetera. So, what we did is we start with that base model, it didn't know COBOL very well, and what we did is we put a team who used the methodologies that was available then -- fine tuning and we used some extended pretraining as well -- and we taught it COBOL. And we did this through this process of iteratively, I think it was we did 14 generations of them around that to improve the model. Then when the new methodology of InstructLab was invented over the Christmas holidays of this year, in February or so we asked the team, hey, you know, we have that project where we have been fine tuning and improving the model to teach it COBOL, how about we try InstructLab on that project? So, we did kind of like an A/B experiment kind of thing. And so the team took that challenge on, and they took the base model and they gave it foundational knowledge -- so they gave it eight COBOL public books, one mainframe COBOL programming manual and one Java public book. Okay? So, they gave it that foundation knowledge. And they gave it some examples of skills; so for example, examples of how you go from COBOL to Java, right? They gave it a bunch of examples, good knowledge around that. And using the InstructLab methodology, you saw that you use a teacher model to then generate a massive amount of synthetic data of examples. In this case, the system automatically generated 200,000 examples and used that to improve the base model. So, they did that experiment; and in one week, they were able to build a model that was significantly better than the model that we had worked so hard over all those months to fine tune and get to that level. That's really impressive. So, the methodology... [ APPLAUSE ] So, improved by 20 points, for example, the code generation ability. But I just want to show you in contrast that today we all are creating value with RAG, we are all creating value with fine tuning. Just see how much value can be created through these principal engineer methodology that is enabled by InstructLab. By the way, once you do this, if you do it with this, RAG will get better and fine tuning will get better. So, it is actually a very powerful and important path to continue to evolve. So, the story for us continues is that we created that, and then now we are able to bring it into the product, we are able to bring that innovation into the world. And here now you see an example into the Code Assistant for Z where you're taking the COBOL script and you're saying, hey, explain to me what the code does. And voil! On the right-hand side, it gives you the explanation of what the COBOL code does. So, quite amazing to see the possibilities that you can do. And this is also Open Source, the methodology. You can find it on github.com/instructlab. [ CHEERS, APPLAUSE ] So, two crucial innovations to bring you in your mission, and the final step is we've got to help you get there. We've got to productize all of this capability and support you and collaborate and create with you to make it happen. So, what have we announced of how we're bringing them to market? And I want to simplify as much as possible for you of what that core platform looks like. At the core of the platform is the announcement that was made at the Red Hat Summit also here in Boston a couple weeks ago, which is RHEL AI, Red Hat Enterprise Linux AI. And we thought very hard together collaborating with Red Hat, what is the simplest, most core capability that we could provide for this new age of AI to a developer? And in the end, we ended up with this connection of, it's a bootable model runtime, a bootable Linux kernel; Granite, the family of models, with full indemnification -- so, the vessels -- and InstructLab as a methodology that a single developer can start trying this business of adding skills and knowledge to the base model. One developer, one laptop. From there...and by the way, and if you're not even ready there, you can just live and try the open-source world, but this is a supported version of it with indemnification. The moment you go from one developer, one laptop and you need to scale it up and you need to have more GPUs, more CPUs, and you know, an environment that allows you to optimize inference across different runtimes, et cetera, and deploy anywhere, wherever you want to deploy this capability, you have Red Hat OpenShift AI. And once you've done that and you're happy with the model capabilities that you've created and you're encoding more and more data into it and now you go into the fact that you've got to say, okay, now I want to take my version, your version of the Granite models and I want to deploy it inside applications, that's where watsonx shines. You're able to now create the patterns to bring them into assistants, to bring them into agents -- which is of huge important direction for the field -- and to provide governance and give you maximum flexibility. So, let me close with a quick reflection and a challenge. The reflection I have is that one of the things that I'm most proud of, and I've had the fortune now of being in IBM for 21 years -- right after grad school, I joined -- and this, we're lucky to be living in this time. This is an incredibly exciting time in the world of computing. What is happening pushing the frontiers of semiconductors, AI and quantum is exhilarating. And what I'm most proud of of the team that is bringing all of this innovation, is the way we're driving organic innovation in IBM is the best I have seen since I joined the company. Our ability to bring breakthroughs from Research and to commercialize them in weeks or months, it is amazing. Every month feels like a year; however, it is impressive what is happening. [ APPLAUSE ] And to close almost where I began, that the future of AI is open, no matter what some say. Last year, to not only prove this point but to bring a community of institutions together, IBM and Meta launched the AI Alliance, and now you see close to a hundred institutions, some of the world's great universities, startups, medium size, large scale companies, science agencies, philanthropies from all over the world coming together to say loudly and clearly, we, too, are going to contribute to the future of AI. We have talent, and we're going to invest R&D and our capacity to create AI, to govern it, to make it safer, to make it more capable, to reflect the diversity that our societies have, expressing our institutions. And that that future that we're going to build together, we're going to build it through open innovation. And that is a future that you should have total confidence that is the winning strategy for your business, and it is the right way to build AI in a way that supports the needs of our society. So, I want to thank you for coming and spending these days with us. And to all the clients, to all the partners who are here with us, and also to all the IBMers that are making this possible, I really could not appreciate more, we could not appreciate more. We know how valuable your time is, and to spend this quality time with us is priceless. I know the show is not quite over yet. There are a few more talks in the afternoon, and I encourage you to go check them out, and all the fun activities that we have. So, enjoy it, and I hope to see you soon. Thank you. Thank you. [ APPLAUSE, MUSIC ]
