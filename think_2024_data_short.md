Good morning. Good morning, everyone, and welcome to Think 2024. It's wonderful to see you all here, and it's incredible that so many business leaders, technologists, innovators are here, are here so you can all learn from each other. When we look at Think, Think is really meant for you, our clients and partners, to help you co-create with each other and together with all of the technologies that you going to see. and I do hope you'll get a chance to walk around the Expo floor where there are so many of our team members and demonstrations there for you. I think this is a critical moment in technology. When we look at the confluence of hybrid cloud, artificial intelligence, quantum computing coming down the road, all of these are technologies that are reaching maturity that can really impact the business and really help the business improve. They can boost productivity, they can bring you automation, they are bringing you innovation, they can help your business at scale in a way that couldn't even be imagined a few decades ago. We're really proud of our technology Atlas -- a concept we introduced last year -- where you can begin to see how these technologies are going to evolve, and you can fully experience that at Think on the Expo floor where we're going to show it to you and show you how much progress that we have made. Now, the topic that is really in everyone's mind, artificial intelligence. If we look at the history of the world on technology, every generation...well, maybe every half a century or so we have had a fundamental technology that has helped the world really advance. You can look at examples: the steam engine, or electricity, or the most recent one, the Internet. What did they do? They made global GDP go up; by the way, with the steam engine in about a century, it went up by 10 times. Again, if we look at electricity, the same thing happened circa 1870 to 1950. If we look at the Internet, it is well on its way there; this time, looks like about 50 years will get us that 10 times increase. Artificial intelligence should be thought about in the same way as those fundamental technologies. It's only going to take a few short years, and we will find it infusing everything. The current estimates is $4 trillion of annual GDP productivity by 2030. Just think about that: over $4 trillion associated to one technology. Now, that's not the technology spend, that is the value accrued to all those that are using it. What's needed to make this come real? There's a lot of experimentation that's going on. That's important, but it's insufficient. If you watch every one of the previous technologies, the history has shown you kind of move from innovating a lot to deploying a lot. As you deploy it is when you'll get the benefits. But in order to deploy, you also need to start moving from experimenting to working at scale. Think about a small project. Then you've got to think about it in an enterprise scale in a systemic way. How do you begin to expand it? How do you begin to make it have impact across an enterprise or across a government? And that is what is really going to make it come alive. When we look at the stage right now, incredible excitement in the consumer world. We see these apps, we see these APIs that are able to impact hundreds of millions of consumers. And I think that's wonderful, helps the technology advance, gets people used to it. You can see the excitement. I kind of think of it as equivalent of maybe browsers, since I'm using the Internet analysis. But then, if you think about it in the Internet analogy, really unlocking productivity inside an enterprise, how do you really connect everything from your inventory to your supply chain all the way to the front end and omnichannel? How do you begin to connect all those systems? AI is going to have the same impact. It's going to become deployed across the enterprise, unlocking and unleashing a massive amount of productivity. For the first time, we will have assistants that really augment humans able to work together with us, unlocking all this. So, what do we need to be able to unlock all these benefits? And if we think about the scaling, that is why we introduced watsonx last year. If you remember, we talked about it and we launched it in the month of July last year -- so about 10 months in market -- and it really is aimed at helping you accelerate your AI deployment. Because why? You need an environment where you can trust, you can play with models, you can experiment with models, you can fine tune models, you can add your own data and skills to models. As our team tells me, a model, in the end, is just a representation of your data. So, in some sense, all your IP is getting locked up, but in a slightly different form, so you need that trust to decide where are you going to run these things, where do you train them with your own data, how do you make sure that only trusted data, what kind of guard rails do you want on it? All of those pieces are going to be there. We also want to make sure that you can leverage smaller purpose tuned models. Why do I say that? When we are looking at the energy, at the cost, at the overall environmental impact of running very large models, the question comes up, can a small model do as much maybe using one percent of the energy and one percent of the total cost? No compromise on quality, no compromise on accuracy? If we can do that, then there is a massive advantage. Of course, I'm saying "if," because we are a hundred percent sure with some of our learnings that that is possible, and I'll touch on one example in just a minute. So, can we control those skyrocketing inference costs but maintain the quality and performance? And also, in many cases we want to bring expertise, bring you people who have worked on many projects before that unlock massive value for the enterprise and bring you their expertise to deploy it at scale inside, touching on the technologies, but also some of the people changes that are needed in order to embrace all of this. The use cases that we see going across our thousands of clients fall into three buckets. They fall into customer experience, into code -- as in, programming -- and into digital labor. Sort of some quick examples to make them come to life. When we think about coding for a moment and we think of what we're able to do ourselves in some cases for Java, for COBOL, for Ansible, our teams are able to get 60 percent of productivity, but I'll be cautious: in the programming task. The remaining parts that still touch the business that get requirements are still very much all human. But 60 percent productivity in half your time I think is a pretty good number. One of our clients, Broadridge, used these tools to improve an application -- they're in the fintech space -- and they saw their programmers get a massive amount of productivity as they evolved forward. When we think about customer experience, rather than ourselves, one of our clients -- in this case in Europe, a telecom provider -- was using Watson to handle 800,000 calls a month. What they found is 40 percent improvement in customer satisfaction means people were happier interfacing with the artificial intelligence -- in this case, powered by watsonx, -- but also 30 percent time reduction on average in terms of how long it took for people to get their answer. So, when we think about these great examples on code, great examples on customer experience, and I think with digital labor, our own human resources center managed to get now 95 percent of the queries -- what we call the transactions that people ask -- get handled between automation and AI. I think these are great, great, great examples. So, the first piece of news that I want to talk about, we decided that we are going to Open Source our Granite models. So, Granite is a family of models that IBM is creating. These are large language models that we have invested in now, and we have them in language. We have them in code, and I'll touch on the code example in just a moment. We have them for time series. We have them for climate change. We're beginning to put them out for cyber. And when we say language, there's many languages that we will do. But we decided that we want to Open Source this. They are ranging anywhere right now from three billion to 34 billion parameters, so that's kind of the range they're in. There isn't a fundamental limit. They could get larger, they could get smaller. We believe that actually having a smaller model, but that is tuned for a purpose -- so, rather than one model that does all these things, if we can actually make models that are fit for purpose, then we can give you the best of both worlds. What do I mean by the best? Look at the leaderboards to say, how good are they? So, we take pride in how good our code model is. It's up there on the leaderboards. It'll go match the best in the world, regardless of how big they get to be. And that's kind of our commitment, and we have 116 programming languages already in there. But then on languages, we'll do some ourselves, but we'll also embrace those who have their own great languages. And so these are put out under the Apache license. Why Apache? So that those of you who want to add a skill from your own data and your own domain knowledge, then the Apache license allows you to keep that to yourself, and you don't have to share it unless you want to. But if you want to share it, then it can become shared, and everybody can benefit from it. Now, other than this, we are also going to be bringing it into a lot of assistants. We have the watsonx Assistant for Enterprise Java that's going to be coming out later this...late in the summer. We have the watsonx Assistant for Z, which allows people with no skills on Z to very quickly learn and get productive on that platform. We have the watsonx Assistant for Code, in which case this is one that is on COBOL, another one that's on Ansible. And also an Assistant Builder so you don't have to rely upon the assistants that we provide, you can actually build your own assistant very quickly. Anybody interested? Go look at them on the expert floor. All of these are out there, and our teams will be excited to show you what to do over there. Now, as the environment becomes bigger, everybody is using multiple public clouds, people have SaaS applications, people have their own in-house applications. With generative AI, estimates are between 600 million to a billion new apps will be written by the end of this decade. Just think about it. So, think about all of that sprawl and all of the connectivity and all of the infrastructure under all those. We feel that it's time to bring AI to IT operations and really help you get a sense of what's going on, what is happening when something goes wrong. Can you judge your response time? Can you take care of problems on their own as opposed to putting humans in the loop? And that is our vision behind IBM Concert. Starting step, looking at all of the security implications on the environment I just described, where do you need to patch, where are you getting something missing, what do you have to go fix very quickly? But from there, the whole vision of what I described is what you are going to see us rolling out over the next many months. My final piece of news around our ecosystem. This is a place where we put a lot of investment and we have really focused on both partnerships and integrations. If you look at what we're doing with OpenShift and watsonx with Adobe Experience Platform, I think that's a great integration. We're going to talk a little bit more about that later. Our Consulting teams are also doing a lot of work on Adobe Express. When we look at AWS, we are integrating the watsonx family with AWS SageMaker, especially for governance -- another great, great example of partnership which is going to help drive business, I believe, for both AWS and IBM. watsonx runs on Microsoft Azure, the whole platform. It is in the Azure Marketplace, so that's a great example of what is there. Also, many other IBM products in the Azure Marketplace. In the case of Meta, Llama 3 is now available through watsonx with indemnity back to our clients. That's a great example, again, of us coming together. Mistral -- a partnership done so that the large Mistral models can be purchased through watsonx. SAP, ServiceNow, Salesforce, integrations of our Granite models with all three of these platforms, that's all been announced or getting announced very quickly. And my last example, with the Saudi Data and AI Authority -- SDAIA. SDAIA did a lot of work to create a wonderful Arabic model, ALLaM. And we have just recently signed an agreement to bring ALLaM, the Arabic model, to watsonx. So, joining me on stage to talk about that, I'm honored to have His Excellency, Dr. Esam Alwagait, the Director of the National Information Center at SDAIA, to join me and we'll talk about the ALLaM model. [ MUSIC, APPLAUSE ] Esam, welcome. Welcome to Think 2024. ALWAGAIT: Thank you very much. I'm very glad to be here in such a remarkable event. And we're very thrilled with the announcement of integration of our Arabic LLM, ALLaM, into watsonx. KRISHNA: So, Esam, maybe you tell the audience a little bit about SDAIA and SDAIA's mission. ALWAGAIT: Arvind, we stand at the edge of a new era. The transformative power of AI is evident around the world; and in Saudi Arabia, we realize this more than ever. Our vision is to harness the potential of AI, to drive economic diversification, to enhance government efficiency, and to provide quality of life for all. In the Saudi Data and AI Authority, we stand at the forefront of Saudi's AI endeavors. We look at a wide spectrum from innovation, to regulation, to operation, alongside a wide range of national ecosystem and partnership. To achieve our vision, we develop and operate critical government digital services and infrastructure that stands on top of a national data bank -- a bank of interconnected national data platforms that aims to increase data sharing, data quality, and to instill data as a main driver for the digital economy in Saudi Arabia. Arvind, let me give you one example of our product. It's called sawtuk, which means "your voice." Sawtuk understands accurately more than 20 dialects in Arabic, and we plan to add more languages in the pipeline. This will improve the human-computer interaction. There is a limited, if not a lack of, focus on Arabic language in LLMs, and that was a call for action. And our action was ALLaM. KRISHNA: That's great to hear. So, when we think about this announcement and what you have done with ALLaM, how will ALLaM and watsonx coming together contribute to your mission to be a global leader and to fulfill the objectives that you just set out? ALWAGAIT: Well, watsonx has many capabilities; for example, enabling LLM development, simplifying machine learning operations, providing the flexibility for hyper cloud deployment. And now with the addition of ALLaM -- the Arabic LLM -- into the platform, we believe that this is a major leap of our goal to becoming an AI leader. In SDAIA, we have many AI products, but ALLaM is our gen AI solution that focuses mainly on Arabic language. We know that rich LLMs are the basis of successful gen AI deployments; however, current LLMs have some limitations when it comes to Arabic language. That's why we provided ALLaM. ALLaM was trained on more than 500 billion tokens in Arabic, and this is just the beta version. We'll continue improving. We'll continue training it to more Arabic data sets. We'll continue to fine tune it, to reach our goal, to have ALLaM as the leading Arabic LLM model in the world. And we know, Arvind, that IBM has a reputation of having high standards when it comes to quality assurance, and we're very happy that our product has passed your QA tests. KRISHNA: It's great to be together on this, but I think you'll agree... [ APPLAUSE ] But also on the other side, who could have a richer trove of Arabic literature and material than Saudi Arabia? ALWAGAIT: A hundred percent. A hundred percent. KRISHNA: So, what is your vision for our partnership going forward? In what ways should we collaborate even more? ALWAGAIT: Well, SDAIA's an IBM's partnership as a strategic one, and it has proven itself over the decades, and today's announcement is a landmark in this strategic partnership. When it comes to AI governance and ethical AI, we share the same priorities and concerns with you all. In SDAIA, we pay huge attention to ethical AI. That's why we launched our second version of Ethical AI Framework, to enable people and entities to develop AI-based solutions that are safe and trustworthy. We're also present on the global stage and we collaborate with many international entities; for example, we've collaborated with the Global AI Advisory body in the UN to reach a vision in a more governed AI. We also got the recognition from the UNESCO's General Conference that granted the International Center for AI and research in Rihyad the status of a Category 2 center. Also, SDAIA's president, Dr. Abdullah Alghamdi, participated in the AI Safety Summit 2023 in the U.K., trying to reach to a more common understanding of AI risks and to have an interesting collaboration on AI safety matters. Arvind, watsonx has AI governance built in it. As we progress and develop more guidelines, more regulations, more standards in AI, it's very important to have the ability to enforce them through code, and having this ability and new platform is a huge advantage. KRISHNA: So, first of all, Dr. Esam, thank you so much for partnering with IBM and all of the collaboration actually over the decades that we have done together. Any final comments from you? ALWAGAIT: Well, I would like to thank you, Arvind, and thank IBM for allowing us to participate in this marvelous event. And I would like also to thank His Royal Highness, Prince Mohammed bin Salman bin Abdulaziz Al Saud for his unwavering support for SDAIA that enabled us to develop AI products, and today's announcement is the result of that support. I would like to invite you and everyone here to attend the third edition of the Global AI Summit in Riyadh in September this year, where we have the pioneering innovators, the pioneering policymakers, the thought leaders to explore new horizons in AI and to forge meaningful collaborations to advance AI forward. Thank you very much, Arvind. KRISHNA: Thank you. Thank you. [ APPLAUSE, MUSIC ] Let's thank Dr. Esam. [ APPLAUSE ] I think that's a great example of a rich LLM and two people who trust each other coming together to bring all the innovation to the world. But to go a little bit deeper, to get the productivity that gen AI promises, we need three things: one, we need to trust the underlying artificial intelligence, and you heard a little bit of that with Dr. Esam. But that is the reason that we together with Meta help form the Open AI Alliance where more than a hundred organizations have gotten together across industry, academia and government in order to help form standards and tests so that we can trust AI. We also need AI to be flexible. How do we combine models? Models should come whether from parties like us, whether from other parties like SDAIA, whether from other commercial entities, whether from Open Source. And you should all have the flexibility to deploy these models where you see fit on a public cloud of your choice, on premise, with sovereignty or in a large public environment. And so how do you get all of that flexibility? And lastly, safety is critical, and we fundamentally believe that safety comes when many eyes can look at something, otherwise called Open Source. So, how do you really get many, many more minds on this which brings you both safety, but also innovation? That's one of the reasons that we've been pushing on bringing a lot of capabilities around AI inferencing and deployment into Red Hat Linux, and Red Hat Linux has a lot of the capabilities on how do you form a platform to run these AI models and run inferencing tasks across many different hardware, but optimized to the hardware so that you can extract 10 times more performance than you might be able to get otherwise. We include the Granite models, but we also include many of the other open source models that I just talked about. But then you get the question of, how do you begin to add skills? And I'm using the word "skills" purposefully, and capabilities. How do you infuse some of your own proprietary data and knowledge to add a skill where a model that is built may not have something that is for a particular purpose. You could imagine cases around certain languages, around code and so on. So, you need to infuse it, but you need to do it quickly at much lower cost than other approaches, and that is why we are bringing InstructLab to the market also. Now, rather than me talking about InstructLab, let me bring out one of the software engineering managers from Red Hat, [Mo] Duffy, to talk to all of you about InstructLab. Mo, welcome on stage. [ MUSIC, APPLAUSE ] So, Mo, thank you for joining us. Can you start by telling us on InstructLab, what's new from a client's perspective? DUFFY: It's a great question. So, when I like to understand how a technology works, I like to start with the client problem to solve. So, we'll take an example of an insurance company. Here we have, whoops, look at all that red. Those are insurance claims waiting on repair recommendations for an auto claims service. Now, this insurance company has lots and lots of historical repair recommendations for claims in their databases. Why can't they take advantage of that and see if we can improve that, get that red back to green? So, what we ask ourselves here, can we use AI and teach a model all about those historical repair claims that worked in the past and use it to create recommendations for the future to kind of unblock all that red? KRISHNA: But how do you get all that knowledge into the model? DUFFY: That's a great question. So, we have the lab method, which was invented by IBM Research, and this is the engine that drives InstructLab, which Arvind has already introduced to you. So, InstructLab has four components. The first component is a taxonomy based tree of knowledge -- so, as Arvind said, we have skills that we want to teach the model. We'll use the client's knowledge, infuse them on top of an open base taxonomy tree. That will be used to do synthetic data generation, we'll talk about that a little bit later. Once we have that synthetic data generated, we will critique the data to validate it. And then once we have that data set, we'll use that to fine tune a model. That model will be fine-tuned with the client's data, and they will have their own version of an open-source model based on their knowledge. KRISHNA: Sounds really, this thing, you used this word "taxonomy" a couple of times. What's a taxonomy, and how do you augment a taxonomy? DUFFY: Sure. So, basically, we're going to take...the taxonomy is a tree of knowledge. This is opposed to the blender method of AI training, where you just take all the data that you can suck in and throw it in a blender and see what comes out. This is a more ordered form of constructing your data, which will help you identify any gaps in the data. What we suggest doing is taking the open community, the innovation that the community has provided us, use that as a solid base; and then, the customer can create their own private version of the tree using that base, infusing their knowledge into it. So, what we do is, for our insurance company, we will take their claims data and create a question and answer file. This is basically a quiz that we use to teach the model all about, what are the best repair recommendations to make for this type of car, that type of accident, that kind of stuff. And then we have a custom tree built just for that client. KRISHNA: So, do I remember this right, so sort of an engineering approach, but also a lot faster than prior approaches of what we'll call fine tuning. Is that accurate? DUFFY: That's right. So, what we're doing here is, you know, this is an insurance company, they have a certain amount of repair claims data. They might not have enough data to actually move something as large as a language model. So, what we do is we amplify the data that they have using the synthetic data generation technique. This way, it makes it faster. We don't have to rely on human labor to expand the data set to move the model. It just processes that using the algorithm that IBM Research came up with. So, it's a cost-effective way to move the model. Once we have that data set, our imaginary insurance company here will have their own custom repair claims data recommending model; and then once they have that model trained on their data, they can add that into an app. Here we have an application that is meant for insurance claims agents to suggest repair recommendations to them. And here we go, it's making a great recommendation based on the repairs data that we have. We've unblocked all of that red, getting it into green. Our client can better serve their customers. And this is all a great example of the power of open. We have open-source licensed models, we have open-source technology in InstructLab, and we have that open taxonomy base for the client. So, all using the power of open. KRISHNA: All open and months of work converted into a few days there. I think that's a great, great example. Mo, thank you so much for sharing InstructLab with us. DUFFY: Nice to be with you. [ MUSIC, APPLAUSE ] Months to days, I think that's the kind of innovation we all want to see in artificial intelligence. So, I wanted to start sharing some real world success stories, and the first guest that I have coming out is going to talk to us about some of the incredible work that Citi, a global bank, is doing around AI. And I want to discuss the evolution of AI as Citi under the leadership of Shadman Zafar, who is the Citi Co CIO, and he's responsible for technology and digital ops behind all of Citi's customer solutions. Shadman, can I welcome you to the stage? [ MUSIC, APPLAUSE ] So, Shadman, it's a real pleasure to have you here with us. To start, could you give us a high-level overview of Citi's AI strategy? ZAFAR: Of course, Arvind. First of all, good morning. It's an absolute pleasure to be here. What a show, by the way, what a show. It's fantastic. Talk about AI strategy for Citi, let me start with an obvious. So, Citi is a large bank, global bank. When you're a large global bank, what you do, you sell money, and the great thing about selling money is your product is always in demand. You know, if you don't have money, you want money. If you have money, you still want more money. [ LAUGHTER ] So, when that happens, how do you differentiate? I mean, it's a great thing, but how do you differentiate? It really comes down to three things: number one, responsibly acting towards your money. That's about risk and control and governance, how we manage money well and how we earn trust from our customers on a daily basis. That's the foundation of our business, and that's one of the pillars about AI strategy. Second, friction: how easy it is to do business with us. More friction, harder for us to do business with, you can go to somebody else for that. Third is a scale. Banking is a scale business. So, our AI strategy is built on these three principles primarily. First and foremost, we are focusing on utilizing AI to really enhance, automate and strengthen our controls. In a financial industry and for a large bank, you're going to have tens of thousands, hundreds of thousands of controls at any given moment in time, monitoring, tracking, managing, remediating issues that may happen. And imagine taking those with AI, and we are looking at making them automated, enhanced, complete, robust. I'm a big believer in having not thousands of controls but a few good controls, and AI is really helping us do that. Second, as I said about friction. We are looking at both for ourselves and how we service our customers, taking friction out by simplifying our processes, automating activities that are generally not easy to do with very brittle processes, if you will. The third, as I said, scale. Many companies have taken the approach, Arvind, in AI of like, let's figure it out, we're going to put it in a lab and do something. We don't think about it that way. We have this mindset, we say take AI out of the lab on to the factory floor, so we have deployed it to tens of thousands of people to use it in everyday capabilities, if you will. And those three pillars of strategy, if done well, have a side effect. If done well, you get a massive productivity, because if you take friction out, you automate controls, you obviously get a huge productivity and we sort of turbocharge our employees with that. KRISHNA: I love your point of scaling. I think it is so, so important that our audience also recognizes that unless you scale, the full benefit of AI is not going to come. You'll get some, but not the full. ZAFAR: That's exactly right. KRISHNA: So, tell us a little bit about the collaboration between Citi and IBM and what kind of solutions you think we're working on together. ZAFAR: Sure. So, you know, Citi and IBM relationship is deep. It's actually deeper than I even realized recently. It goes back many decades, I think to when I was still in high school or probably middle school. And before I was coming here, I was actually looking and trying to look for some examples of where we don't work together, and I literally couldn't find one -- because I was trying to find an example to say, and this is a new area we'll work on. But be it AI, cybersecurity, cloud computing, enterprise computing, legacy systems, modern systems, even business processes. We worked with IBM for a long time, long before my time, by the way. I think, Arvind, long before your time, too. KRISHNA: Probably. [ LAUGHTER ] ZAFAR: We have worked together well. So, bringing that to...so, because of that kind of deep relationship, obviously we're deeply intertwined in our AI path as well. We are very focused on AI. I'll give you a couple of examples how we are working together. So one, let me start by sort of hardware side, and then I'll talk about business processes. So, again in banking, fraud is a big area from a compute perspective. You have to figure out there are, you know, millions...sorry, billions, tens of billions of transactions happening every day. Figuring out what happens, detect the fraud, and then do something about it. It's a very near real-time activity we need to take place. The way it happens today, Arvind, is you have to collect all of that data, you have to offload it for like a machine learning algorithm to execute, and then bring that data back on after there is a score to take action about it. In the case of fraud, that velocity matters. So, one of the key things we actually worked with IBM is IBM's new Telum which has AI onboarded, and it executes with a very significant inference capabilities while...I mean, the interesting part was, at least for us when we were working with your team, is to really see that we can actually onboard and offboard in real time AI inference execution and sort of traditional processing all together in one play. And we have now convinced ourselves can actually do this whole thing in one place in real time. KRISHNA: There was skepticism about it originally. ZAFAR: I mean, I've got to tell you, I was a skeptic. I was a skeptic, my team was a little bit of a skeptic. And it was fantastic to see we can do that in real time, not offload it, and also train on production data. So, that's fantastic. I mean, that is I feel is like a step function change for us. But also we are working on, we are an Ansible shop. I mean, you talked about Ansible. We're an Ansible shop. And a company like Citi, you know, at any given moment in time there are tens of thousands, hundreds of thousands of changes happening on our infrastructure every week. And we are a mission critical company. So, we are actually working on Ansible with watsonx integration to really automate that thousands and tens of thousands of jobs, make them sort of automated, real time, and improve the quality of our technology operations. And I'll just give you one last example because this sort of touches a different area. As you know, Arvind, we use OpenPages from IBM for our risk and control governance platform. And again, with the watsonx integration coming in place, we are actually building now a new set of tools to use AI to write in from auditing perspective, audit report, reviewing, figuring out the risks, recognizing the risks in real time, and being able to take action on them. So, there's a long list of things... KRISHNA: I really love those examples. They do play to your scale point. I mean, audit, if I remember, you have four, five, 7,000 people in those departments. ZAFAR: That's very true. KRISHNA: And on code deployment, there's tens of thousands of engineers who work for you and your teams... ZAFAR: That's correct. KRISHNA: ...who get impacted. I think that these are great scale examples. So, maybe as a final question, are there other use cases you're considering; or kind of, you know, I'm sure you get asked this question by your boss, what's up next? ZAFAR: [CHUCKLES] Every day. Every day, seven days a week. So, look, first of all, as I said, we are approaching AI not as in a lab two use cases or five use cases, we are really looking at it as a new technology stack. So, we are looking almost everything we do through that lens, a new technology stack; and because of that and thousands of programs we look at, that sort of approach through AI lens, if you will, Arvind. But to maybe take a step back, first of all, Citi has been utilizing AI for some time. I mean, even before sort of generative AI energy that came into the market, we have hundreds of sort of traditional AI models in place to do fraud detection, natural language processing to respond to our end customers. We even use like AI-integrated end device protection. We have like three quarters of a million devices that are protecting ourselves, utilizing AI against ransomware and whatnot. But yes, to give you a sense, at a high level we look at major patterns. So, four patterns I will highlight, is like a pattern around smart query. So, we are trying to get most of the corpus of corporate data into sort of a RAG environment that you can in a very simple way ask questions. And it can be used multiple ways: internally to create documents, to produce reports, to whatnot, but also externally to provide services to our end customers. Many times our front end employees are sifting through thousands of documents to actually figure that out. So, that's what we call smart query. Then you have sort of the smart content generation. Again, for internal use and external use, they're using generative AI to produce sometimes external reports, sometimes internal documents to sort of create and analyze things. Then it's around really engineering itself -- so, code modernization -- and there's a long list of things there, actual code assistants and code writing but also taking old systems and modernizing them to new systems. By the way, those are projects that will not just financially but logistically impossible to undertake how complex and how old the systems were that we can undertake now. I've tried it seven, eight times before, and I always fail, and I'm finally actually seeing the success happen, and it was not because of lack of money, because it was just logistically so hard to undertake a project like that. And the last I would say -- and that's probably the most common we all hear about -- is like, you know, a set of what you would call probably copilots or general assistants, AI-based assistant that goes across the corporation for specialized functions for people. KRISHNA: I love what you talked about, you talked about a new tech stack, you talked about scaling, you talked about the user generative. But it struck me, you also gave an example that you can now do something that actually even with money and people before you would not have agreed to tackle but this technology makes that possible. ZAFAR: Absolutely. KRISHNA: I know exactly what you're talking about, when there's a spaghetti mess of 10 million lines of code, how do you unravel it and do something with it? I think those are great examples. Shadman, thank you so much for being here... ZAFAR: All right, thank you. KRISHNA: ...and sharing your story with us. ZAFAR: Of course. Thank you. KRISHNA: Thank you. [ APPLAUSE ] Look, as we think about the example from Citi -- and I hope that you all got some great ideas of what you can all do -- I think it's time to maybe also talk about one of our great partnerships, a partnership that was founded on strength but has been strengthening again and again, and that is with Adobe. IBM was an early, early adopter of Adobe Firefly, and I will tell you that our teams love it. When we think about what our teams were telling me in the middle of 2023 as that were using it, this is a great example of how AI is strengthening a product that they liked already. By the way, it's not just our teams inside IBM, though they love it, but our Consulting teams have also been deploying the Adobe Experience platform, and they're also creating new service offerings for Adobe Express. So, to help talk about all this, please join me in welcoming Shantanu Narayen, CEO and Chairman of Adobe, to the stage. [ MUSIC, APPLAUSE ] Shantanu, thank you for joining us here. NARAYEN: My pleasure. Thanks for having me. Wow, quite an audience here. KRISHNA: It's a great audience, testament to the interest in AI, I think, actually. So, Shantanu, thank you again. So, the Adobe/IBM relationship has seen significant growth over the past few years. Any comments on that, and what do you see as kind of what's behind that? NARAYEN: Well, again, thanks for having me. And as you said, I think we as companies have a passion for how do you have technology and how do you put technology to bear to impact people's lives. You talked a lot about productivity, Arvind. I mean, certainly Adobe perhaps focuses a little bit more on creativity. And I think, you know, what we have done together, a lot of people...for Adobe, we have really three key businesses. We talk about enabling people to create content, we talk about the role that PDF plays as it relates to information and how you can have knowledge management. And perhaps a business that people know us less for is powering digital businesses. So, as you know, we were one of the first companies to say we're really going to focus on marketing and how do you enable people to have those personalized digital experiences. In all of these areas, we've been really thrilled to partner with IBM. IBM was the first company to also talk about how they would be a global partner for us with Express and really getting Express, which is a creativity tool, out. But I think the thing that we've probably done the most pioneering work in together is content supply chain. We all know the world's producing way more content, so really thinking about how do you create that content, how do you deliver that content, how do you monetize that content? You've announced a global partnership with us on that, so it's been really exciting. But I think the best is yet to come. KRISHNA: Look, I think it's exciting. I think what you just related, Shantanu, is that we have a combined mission of digitizing, in this case, I'll call it the creative supply chain inside an enterprise, even though we market to enterprise customers. And being able to share that vision, I think is behind some of the success we have had. Now, another area other than AI that we both have a belief in is the technology called hybrid cloud. Would you talk a little bit about how Adobe has been adopting Red Hat OpenShift and how it empowers you with your clients? NARAYEN: Absolutely, Arvind. I think you talked a lot about, you know, if you think of the stack that exists for AI, you talked about data and the importance of data. You certainly talked about models, and I'm sure we'll touch on that, and interfaces. One of the products that Adobe really focused on a lot was what we call the Adobe Experience Platform. And the Adobe Experience Platform is really a real-time customer data platform that allows you to activate in real time any customer engagement that's happening. So somebody coming to your website, somebody being in a store, how do you really activate that? Now, we know that every one of the customers in this room has a very heterogeneous environment, and so allowing flexibility. You talked about regulated industries, financial services. So, I think it was really critical for us to partner, to understand how we could get the Adobe Experience Platform to work in this heterogeneous environment. And certainly OpenShift was the best way for us to embrace so that we could get it to work, regulated industries, public sector, getting it to work on AWS. So, we're thrilled to partner and have the Adobe Experience Platform available through Red Hat OpenShift on all these heterogeneous environments. KRISHNA: That's great. So, opens up the aperture to get even more business done at the end of the day. NARAYEN: And to really allow all the customers in this room to say we have an environment. IBM, Adobe, you need to partner to ensure our environment is the way we deploy these solutions. KRISHNA: So, look, Adobe has long been known as a leader in artificial intelligence, but how are you integrating more generative AI within your products and taking advantage of this for your clients? NARAYEN: In your opening remarks, you went back perhaps a little bit longer than I would have when you talked about steam engine and electricity, but I certainly agree with you on the impact that this can have. And like Shadman also said, Adobe's been involved, I mean, computers are great at what? They're great at pattern matching and AI. And so we've always embraced every tectonic technology shift to say how we can bring value to our customers. But when you think about creativity, I think the biggest challenge a lot of people have is that fear of a blank screen, right? A blank page. And so we really just think about generative AI. We say that everybody has a story to tell, and if we can enable people to tell their story through our creative products, whether it's Express for every individual, we just think that's going to change everything as it relates to AI. So, Adobe announced Firefly, that's our imaging model. We're very differentiated in that we said, because we represent the creative community we have to make sure that our models, like you said indemnification, we actually indemnify because we say all of the IP on which these models were trained for Firefly are data that we own. So, we have not violated any IP protection. So, we did that on the models. And then in the interfaces, you know, I've been at the company for over 25 years, but the adoption of this in a product like Photoshop where people have said through generative fill we can now just have a conversational interface with Photoshop and do things, it's just mind boggling, the kind of innovation that we've done. So, that's on the creative side. On the marketing side, every one of you is probably saying, how do I provide that great personalized experience to a customer? How do I segment them? How do I acquire them? How do I do geographic variations in different countries? And I think that's where we've really embraced the power of generative AI, to accelerate, you talked about automation, so whether it's automation, whether it's creation or whether it's delivery, we think it's going to be massive productivity gains. KRISHNA: But in the end, products are becoming better, and the clients are able to use them more... NARAYEN: Exactly. KRISHNA: ...is what you're kind of saying. You mentioned indemnification, but the broader topic behind indemnification is really trust, and I think we both agree that building trust behind how we use AI and deploy AI is critical. Which strategies do you think are essential that our clients believe that we are ethical AI producers? NARAYEN: It's a really important topic, because for companies like IBM or Adobe, the biggest thing that we have is the trust that we have with customers. So, a couple of things that we have done on that front, Arvind. First, as I said, the Firefly models, every piece of data that we've used to create those Firefly models is data that we have intellectual property rights for. We certainly have partnered with every company to say, if you have your own data...you talked about how you can create these custom models, so we enable people to create those custom models with trust. But I think taking a step back, we thought a lot about, since we helped create the world's content, what do we have to do? And we created this new concept called Content Credentials. We're fortunate we have over 3,000 companies that are now part of this consortium. And the idea behind Content Credentials, think of it as a nutrition label for a piece of content. And what you can do is for every piece of content, you can specify when it was created, what's the provenance, did AI change it, so that people are assured of...I don't know if people know, 50 percent of the world's population is going to be voting in a 12-month period. And when you talk about all the fake news or content that's created, I think that's one area where we've tried to step up and say, how do we act responsibly? How are we transparent? And at the end of the day, how do we make sure there's accountability for all of the AI work that we are doing? KRISHNA: I've got to give a plug for some of these tools. Our teams have so much trust and get so much creativity out of them, tasks that used to take two to three weeks a year, year and a half ago, the teams are now getting done in a couple of evenings. That really, I think, speak to the power but also the trust that we have. So, Shantanu, any final thoughts? How can IBM and Adobe co-create going forward so that our clients can get even more value from us? NARAYEN: How much time do you have, Arvind? [ LAUGHTER ] I think there's so much we can do together, and we're really excited. I mean, you talked about watsonx. As it relates to Acrobat, let's talk about PDF for a while. We would all I think acknowledge that a lot of the world's information is in PDF. We recently announced Adobe Acrobat AI Assistant, so having a conversational interface where you can query a PDF. You can say, let me get the summary. You can now compare documents to say, if I'm reading two financial reports, what's the difference? I think what you're doing with your models and the ability with watsonx to have native models, ensuring that we can have support for Acrobat and AI Assistant with watsonx, I think that's a big, new exciting area. We all talk about productivity and productivity office tools; creativity, I think, is critical. We're thrilled that IBM has adopted Adobe Express, and you're going to be now a global partner for us to enable everybody to understand how they can create. Redshift, we talked about the things we're doing on the technology with hybrid cloud to enable even more rapid adoption. So, I think across our businesses there's so much more that we can do together, but I think the ultimate test will be delivering value to these customers. So, I think the more we're engaged with everybody in this room to say, are we getting the them the business outcomes, I think that's how we deliver value. KRISHNA: Look, we really look forward to that. Shantanu, thank you so much for sharing what we do together, and thank you again for being here with our audience. NARAYEN: Thanks for having me. Have a great show. [ MUSIC, APPLAUSE ] KRISHNA: Look, I think as you go across what we just heard, this is a really exciting moment in technology. As we look at the intersection of hybrid cloud, artificial intelligence, we really think there's an inflection where every one of you can take advantage of it to help improve your business. I think there's a macro trend that underlies this. Technology has always been used for productivity in automation. Think of it as helping your enterprises become lean. But I think there's a shift that is happening. The role is no longer about just being lean; the role has really shifted to, how is technology powering the business to gain revenue, to gain scale, to get even more share in the marketplace? And that is a big shift, albeit has been a subtle shift underneath. And I think the goal for every one of you should be, how do you embrace technology in the context of your business to help power that shift, not just make yourself the leanest possible? Now, there's a topic that is dear to my heart that I just need to mention for a minute or two, which is we talked a lot about artificial intelligence and hybrid cloud, but another topic that is just coming around the corner is quantum computing, and I think quantum computing holds great potential for what we can all get done. First, a moment of pride. We have built over 70 actual quantum computers, these systems have been deployed globally over the last half dozen years or so. We have 250-plus organizations who participate very actively, hands-on in our Quantum Network, some of you in the room because I recognize you. People have run over three trillion -- three trillion -- individual experiments on our quantum systems over the last many years. I think that speaks to the reality of these technologies. When you run three trillion of something, it's no longer a science experiment, it is something that is becoming a real computer system. As we look upon this, what's the promise? There are problems that classical computers can never solve. I'm making a very strong statement: never. We look at problems around fertilizers, food supply, around new materials, around financial risk, there are issues here that quantum computers will be able to solve in the next three to five years. That is something which really excites us. But there's a booth downstairs. You can go look at it. You can go talk to our experts who are there in terms of what can be done. So, with so much potential, most of you are on artificial intelligence at this conference, but bracketed by both hybrid cloud and quantum computing. I really would like to end by thanking all of you for your trust in IBM, your spending the days here with us. I hope you'll get a chance to interact with each other and learn something which you can take back and implement in your organization. Thank you also to all the partners who help convey all of these capabilities to our clients. But I do encourage you, make the most of this event, interact, co-create, learn something, go back with an insight. Thank you. Thank you all.

